<!DOCTYPE html><html class="theme-next pisces"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0"><link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.5.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"6U6P1RGK4F",apiKey:"b14e73cdd627eabe947b5decbe14850f",indexName:"lsvih",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta property="og:type" content="website"><meta property="og:title" content="My note"><meta property="og:url" content="https://lsvih.com/page/5/index.html"><meta property="og:site_name" content="My note"><meta property="og:locale" content="default"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="My note"><link rel="canonical" href="https://lsvih.com/page/5/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>My note – lsvih</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?4de170e51b04cee02fbdb1b7cb7a8a60";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="default"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">My note</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">lsvih</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="algolia-popup popup search-popup"><div class="algolia-search"><div class="algolia-search-input-icon"><i class="fa fa-search"></i></div><div class="algolia-search-input" id="algolia-search-input"></div></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/29/Solution-Ionic-cordova-resources-couldn-t-generate-splash-and-icon/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/29/Solution-Ionic-cordova-resources-couldn-t-generate-splash-and-icon/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">[Solution] 'Ionic cordova resources' couldn't generate splash and icon</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-29 17:48:00" itemprop="dateCreated datePublished" datetime="2017-08-29T17:48:00+08:00">2017-08-29</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Front-End/" itemprop="url" rel="index"><span itemprop="name">Front End</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>Version:</p><p>Ionic: 3.9.2</p><p>Cordova: 7.0.1</p></blockquote><p>Using ‘ionic cordova resources’ will generate all-size splashs and icons for selected platforms automatically.</p><p>But this method depends on cloud service of ionic, so that when you can’t connect to network, this method would be failed.</p><p>There is a awesome tool can deal with these problem: cordova-resgen.</p><p><a href="https://github.com/helixhuang/ionic-resources" target="_blank" rel="noopener">https://github.com/helixhuang/ionic-resources</a></p><p>This tool base on cordova-splash and cordova-icon, using graphicsmagic to cut pictures.</p><h4 id="Usage"><a href="#Usage" class="headerlink" title="Usage:"></a>Usage:</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brew install graphicsmagick</span><br><span class="line">sudo npm install cordova-resgen -g</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> your-project</span><br><span class="line">cordova-resgen</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/29/使用-AI-为-Web-网页增加无障碍功能/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/29/使用-AI-为-Web-网页增加无障碍功能/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">使用 AI 为 Web 网页增加无障碍功能</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-29 16:55:00" itemprop="dateCreated datePublished" datetime="2017-08-29T16:55:00+08:00">2017-08-29</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p><img src="/images/pasted-183.png" alt="upload successful"></p><p>图为一位盲人正在阅读盲文（<a href="http://usabilitygeek.com/wp-content/uploads/2012/07/Software-For-Visually-Impaired-Blind-Users.jpg" target="_blank" rel="noopener">图片链接</a>）</p><p><a href="http://www.who.int/mediacentre/factsheets/fs282/en/" target="_blank" rel="noopener">根据世界健康组织的统计</a>，全球约有 2.85 亿位视力障碍人士，仅美国就有 810 万网民患视力障碍。</p><p>在我们视力正常的人看来，互联网是一个充满了文字、图片、视频等事物的地方，然而对于视力障碍人士来说却并不是这样的。有一种可以读出网页中文字和元数据的工具叫做屏幕阅读器，然而这种工具的作用十分有限，仅能让人看到网页的一部分文本。虽然一些开发人员花时间去改进他们的网站，为视障人士添加图片的描述性文字，但是绝大多数程序员都不会花时间去做这件公认冗长乏味的事情。</p><p>所以，我决定做这么一个工具，来帮助视障人士通过 AI 的力量来“看”互联网。我给它起名为“Auto Alt Text”（自动 Alt 文本添加器），是一个 Chrome 拓展插件，可以让用户在图片上点击右键后得到场景描述 —— 最开始是要这么做的。</p><p>您可以观看 <a href="https://www.youtube.com/embed/c1S4iB360m8" target="_blank" rel="noopener">这个视频</a>，了解它是如何运作的，然后 <a href="http://abhinavsuri.com/aat" target="_blank" rel="noopener">下载它并亲自试一试吧！</a>！</p><h4 id="为什么我想做-Auto-Alt-Text："><a href="#为什么我想做-Auto-Alt-Text：" class="headerlink" title="为什么我想做 Auto Alt Text："></a>为什么我想做 Auto Alt Text：</h4><p>我曾经是不想花时间为图片添加描述的开发者中的一员。对那时的我来说，无障碍永远是“考虑考虑”的事，直到有一天我收到了来自<a href="https://github.com/hack4impact/flask-base" target="_blank" rel="noopener">我的一个项目</a>的用户的邮件。</p><p><img src="/images/pasted-184.png" alt="upload successful"></p><p>邮件内容如下：“你好，Abhinav，我看了你的 flask-base 项目，我觉得它非常适合我的下个工程。感谢你开发了它。不过我想让你知道，你应该为你 README 中的图片加上 alt 描述。我是盲人，用了很长一段时间才弄清楚它们的内容 :/来自某人”</p><p>在收到邮件的时候，无障碍功能的开发是放在我开发队列的最后面的，基本上它就是个“事后有空再添加”的想法而已。但是，这封邮件唤醒了我。在互联网中，有许多的人需要无障碍阅读功能来理解网站、应用、项目等事物的用途。</p><blockquote><p>“现在 Web 中充满了缺失、错误或者没有替代文本的图片” —— WebAIM（犹他州立大学残疾人中心）</p></blockquote><h4 id="用-AI（人工智能）来挽救："><a href="#用-AI（人工智能）来挽救：" class="headerlink" title="用 AI（人工智能）来挽救："></a>用 AI（人工智能）来挽救：</h4><p>现在其实有一些方法来给图像加描述文字；但是，大多数方法都有一些缺点：</p><ol><li>它们反应很慢，要很长时间才能返回描述文字。</li><li>它们是半自动化的（即需要人类手动按需标记描述文字）。</li><li>制作、维护它们需要高昂的代价。</li></ol><p>现在，通过创建神经网络，这些问题都能得到解决。最近我接触、学习了 Tensorflow —— 一个用于机器学习开发的开源库，开始深入研究机器学习与 AI。Tensorflow 使开发人员能够构建可用于完成从对象检测到图像识别的各种任务的高鲁棒模型。</p><p>在做了一些研究之后，我找到了一篇 Vinyals 写的论文<a href="https://arxiv.org/abs/1609.06647" target="_blank" rel="noopener">《Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge》</a>。这些研究者们创建了一个深度神经网络，可以以语义化方式描述图片的内容。</p><p><img src="/images/pasted-185.png" alt="upload successful"></p><p>im2txt 的实例来自 <a href="https://github.com/tensorflow/models/tree/master/im2txt" target="_blank" rel="noopener">im2txt Github Repository</a></p><h4 id="im2txt-的技术细节："><a href="#im2txt-的技术细节：" class="headerlink" title="im2txt 的技术细节："></a>im2txt 的技术细节：</h4><p>这个模型的机制相当的精致，但是它基本上是一个“编码器 - 解码器”的方案。首先图片会传入一个名为 Inception v3 的卷积神经网络进行图片分类，接着编码好的图片送入 LSTM 网络中。LSTM 是一种专门用于序列模型/时间敏感信息的神经网络层。最后 LSTM 通过组合设定好的单词，形成一句描述图片内容的句子。LSTM 通过求单词集中每个单词在句子中出现的似然性，分别计算第一个词出现的概率分布、第二个词出现的概率分布……直到出现概率最大的字符为“.”，为句子加上最后的句号。</p><p><img src="/images/pasted-186.png" alt="upload successful"></p><p>图为此神经网络的概况（图片来自 <a href="https://github.com/tensorflow/models/tree/master/im2txt" target="_blank" rel="noopener">im2txt Github repository</a>）</p><p>根据 Github 库中的说明，这个模型在 Tesla k20m GPU 上的训练时间大约为 1-2 周（在我笔记本的标准 CPU 上计算需要更多的时间）。不过值得庆幸的是，Tensorflow 社区提供了一个已经训练好的模型。</p><h4 id="使用-box-Lamdba-解决问题："><a href="#使用-box-Lamdba-解决问题：" class="headerlink" title="使用 box + Lamdba 解决问题："></a>使用 box + Lamdba 解决问题：</h4><p>在运行模型时，我试图使用 Bazel 来运行模型（Bazel 是一个用于将 tensorflow 模型解包成可运行脚本的工具）。但是，当命令行运行时，它需要大约 15 秒钟的时间才能从获取一张图片的结果！解决问题的唯一办法就是让 Tensorflow 的整个 Graph 都常驻内存，但是这样需要这个程序全天候运行。我计划将这个模型挂在 AWS Elasticbeanstalk 上，在这个平台上是以小时为单位为计算时间计费的，而我们要维持应用程序常驻，因此并不合适（它完全匹配了前面章节所说的图片描述软件缺点的第三条缺点）。因此，我决定使用 AWS Lambda 来完成所有工作。</p><p>Lambda 是一种无服务器计算服务，价格很低。此外，它会在计算服务激活时按秒收费。Lambda 的工作原理很简单，一旦应用收到了用户的请求，Lambda 就会将应用程序的映象激活，返回 response，然后再停止应用映象。如果收到多个并发请求，它会唤起多个实例以拓展负载。另外，如果某个小时内应用不断收到请求，它将会保持应用程序的激活状态。因此，Lambda 服务非常符合我的这个用例。</p><p><img src="/images/pasted-187.png" alt="upload successful"></p><p>图为 AWS API Gateway + AWS = ❤️ (<a href="https://cdn-images-1.medium.com/max/700/1*SzOPXTf_YQNtFejG0e4HPg.png" target="_blank" rel="noopener">图片链接</a>)</p><p>使用 Lambda 的问题就在于，我必须要为 im2txt 模型创建一个 API。另外，Lambda 对于以功能形式加载的应用有空间限制。上传整个应用程序的 zip 包时，最终文件大小不能超过 250 MB。这个限制是一个麻烦事，因为 im2txt 的模型就已经超过 180 MB 了，再加上它运行需要的依赖文件就已经超过 350 MB 了。我尝试将程序的一部分传到 S3 服务上，然后在 Lambda 实例运行再去下载相关文件。然而，Lambda 上一个应用的总存储限制为 512 MB，而我的应用程序已经超过限制了（总共约 530 MB）。</p><p>为了减小项目的大小，我重新配置了 im2txt，只下载精简过的模型，去掉了没用的一些元数据。这样做之后，我的模型大小减小到了 120 MB。接着，我找到了一个最小依赖的 <a href="https://github.com/ryfeus/lambda-packs" target="_blank" rel="noopener">lambda-packs</a>，不过它仅有早期版本的 python 和 tensorflow。我将 python 3.6 语法和 tensorflow 1.2 的代码进行了降级，经过痛苦的降级过程后，我最终得到了一个总大小约为 480 MB 的包，小于 512 MB 的限制。</p><p>为了保持应用的快速响应，我创建了一个 CloudWatch 函数，让 Lambda 实例保持”热“状态，使应用始终处于激活态。接着，我添加了一些函数用于处理不是 JPG 格式的图片，在最后，我做好了一个能提供服务的 API。这些精简工作让应用在大多数情况下能够于 5 秒之内返回 response。</p><p><img src="/images/pasted-188.png" alt="upload successful"></p><p>上图为 API 提供的图片可能内容的概率</p><p>此外，Lambda 的价格便宜的令人惊讶。以现在的情况，我可以每个月免费分析 60,952 张图片，之后的图片每张仅需 0.0001094 美元（这意味着接下来的 60,952 张图像约花费 6.67 美元）。</p><p>有关 API 的更多信息，请参考 repo：<a href="https://github.com/abhisuri97/auto-alt-text-lambda-api" target="_blank" rel="noopener">https://github.com/abhisuri97/auto-alt-text-lambda-api</a></p><p>剩下的工作就是将其打包为 Chrome 拓展插件，以方便用户使用。这个工作没啥挑战性（仅需要向我的 API 端点发起一个简单的 AJAX 请求即可）。</p><p><img src="/images/pasted-189.png" alt="upload successful"></p><p>上图为 Auto Alt Text Chrome 插件运行示例</p><h4 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h4><p>Im2txt 模型对于人物、风景以及其它存在于 COCO 数据集中的内容表现良好。</p><p><img src="/images/pasted-190.png" alt="upload successful"></p><p>上图为 COCO 数据集图片分类</p><p>这个模型能够标注的内容还是有所限制；不过，它能标注的内容已经涵盖了 Facebook、Reddit 等社交媒体上的大多数图片。</p><p>但是，对于 COCO 数据集中不存在的图片内容，这个模型并不能完成标注。我曾尝试着使用 Tesseract 来解决这个问题，但是它的结果并不是很准确，而且花费的时间也太长了（超过 10 秒）。现在我正在尝试使用 Tensorflow 实现 <a href="http://ai.stanford.edu/~ang/papers/ICPR12-TextRecognitionConvNeuralNets.pdf" target="_blank" rel="noopener">王韬等人的论文</a>，将其加入这个项目中。</p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>虽然现在几乎每周都会涌现一些关于 AI 的新事物，但最重要的是退回一步，看看这些工具能在研究环境之外发挥出怎样的作用，以及这些研究能怎样帮助世界各地的人们。总而言之，我希望我能深入研究 Tensorflow 和 in2txt 模型，并将我所学知识应用于现实世界。我希望这个工具能成为帮助视障人士”看“更好的互联网的第一步。</p><h4 id="相关链接："><a href="#相关链接：" class="headerlink" title="相关链接："></a>相关链接：</h4><ul><li>关注文章作者：我会在 <a href="https://medium.com/@abhisuri97" target="_blank" rel="noopener">Medium</a> 上首发我写的文章。如果你喜欢这篇文章，欢迎关注我:)。接下来一个月，我将会在下个月发布一系列“如何使用 AI/tensorflow 解决现实世界问题”的文章。最近我还会发一些 JS 方面的教程。</li><li>本文工具 Chrome 插件：<a href="http://abhinavsuri.com/aat" target="_blank" rel="noopener">下载地址</a></li><li>Auto Alt Text Lambda API：<a href="http://github.com/abhisuri97/auto-alt-text-lambda-api" target="_blank" rel="noopener">Github repository 地址</a></li></ul><blockquote><p>发布于掘金 <a href="https://juejin.im/post/59a51e91f265da2499603c8c" target="_blank" rel="noopener">https://juejin.im/post/59a51e91f265da2499603c8c</a></p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/22/随机索引算法论文笔记-An-Introduction-to-Random-Indexing/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/22/随机索引算法论文笔记-An-Introduction-to-Random-Indexing/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">随机索引算法论文笔记 An Introduction to Random Indexing</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-22 09:34:00" itemprop="dateCreated datePublished" datetime="2017-08-22T09:34:00+08:00">2017-08-22</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>Sahlgren M. An Introduction to Random Indexing[C]// Methods &amp; Applications of Semantic Indexing Workshop at International Conference on Terminology &amp; Knowledge Engineering. 2005:194–201.</p></blockquote><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><p>文章作者为 Sahlgren M</p><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>本文主要内容分为了 4 段，分别为：</p><ul><li>The word space methodology</li><li>Problems and solutions</li><li>Random Indexing</li><li>Results</li></ul><p>文章从文本空间讲起，简述了使用向量表示词的作用。接着以 LSA 加上 SVD 降维为例，简单说明了传统词向量表示算法的一些局限性（向量维度依然过大，计算代价大等），引出了 Random Indexing 算法。</p><h2 id="Random-Indexing"><a href="#Random-Indexing" class="headerlink" title="Random Indexing"></a>Random Indexing</h2><p>文中描述：<br>• First, each context (e.g. each document or each word) in the data is assigned a unique and randomly generated representation called an index vector. These index vectors are sparse, high-dimensional, and ternary, which means that their dimensionality (d) is on the order of thousands, and that they consist of a small number of randomly distributed +1s and -1s, with the rest of the elements of the vectors set to 0.<br>• Then, context vectors are produced by scanning through the text, and each time a word occurs in a context (e.g. in a document, or within a sliding context window), that context’s d-dimensional index vector is added to the context vector for the word in question. Words are thus represented by d-dimensional context vectors that are effectively the sum of the words’ contexts.</p><p>结合下面的论文提到的解释理解 Random Indexing algorithm。</p><blockquote><p>论文 2 – 熊玮, 白越, 刘爱国,等. 基于改进RI方法的文本聚类[J]. 南昌大学学报(理科版), 2016, 40(5):426-430.</p></blockquote><h3 id="第一步：生成随机索引向量"><a href="#第一步：生成随机索引向量" class="headerlink" title="第一步：生成随机索引向量"></a>第一步：生成随机索引向量</h3><p>为正文、单词生成随机索引向量。这些随机索引向量是稀疏、高维的。随机索引向量的值可以为 (-1, +1, 0) 三种。大多数的向量值都为 0，只有少数向量值为 -1 和 +1。在论文 2 中提到随机索引向量可以使用二元组 $ (d,\epsilon) $表示。<br>其中，$d$ 为向量维度，$\epsilon$为不同索引向量元素数量参数。对于所有文本来说，它们向量空间中出现的 -1 与 +1 的数量是相同的，在 $d$ 确定后由 $\epsilon$ 决定它们出现的数量。<br>令文本集全集为 $W$，文本集的子集（单词）为$\omega _j \in W,j \in {1,2,3,…,n} $，此时生成的随机索引向量为 $ R_{\nu_j} = (r\nu_1^j,r\nu_2^j,r\nu_3^j,…,r\nu_d^j ) $，其中 $r\nu_{h^j} \in {+1,-1,0}, h \in {1,2,3,…,d}$。$\epsilon$的取值远小于$d$。总体来说，+1 与 -1 分别占随机索引向量总维度的概率为 $\frac{\epsilon /2}{d}$，显然有<br>$$\frac{\epsilon /2}{d} + \frac{d - \epsilon}{d} + \frac{\epsilon /2}{d} = 1$$</p><h3 id="第二步：生成文本向量"><a href="#第二步：生成文本向量" class="headerlink" title="第二步：生成文本向量"></a>第二步：生成文本向量</h3><p>根据滑动窗口包含的上下文生成上下文向量，接着根据上下文向量计算文本向量。<br>在论文 2 中，这一步又分为了两步：</p><h4 id="生成特征词汇的上下文向量"><a href="#生成特征词汇的上下文向量" class="headerlink" title="生成特征词汇的上下文向量"></a>生成特征词汇的上下文向量</h4><p>设滑动窗口大小为 2L，则窗口范围为 [-L, L]。记特征词 $\omega_j$ 在文本 $d_i$ 中的上下文向量为 $c^i_j$，则其表达式为：</p><p>$$c^i_j = \sum^{k = L}_{k = -L}Rv_{j+k} * \omega f(\omega_{j+k})$$</p><p>其中 $Rv_{j+k}$ 表示特征词 $\omega_j$ 在窗口范围内共现词 $\omega_{j+k}$ 对应的随机索引向量；<br>$\omega f(\omega_{j+k})$ 为特征词 $\omega_j$ 在窗口范围上下文中共现特征词 $\omega_{j+k}$ 在文本 $d_i$ 中的加权权重。论文 2 中采用了 tf-idf 加权计算算法。论文 2 此时引用了</p><blockquote><p>Gorman J, Curran J R. Random Indexing using Statistical Weight Functions[C]// EMNLP 2007, Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, 22-23 July 2006, Sydney, Australia. DBLP, 2006:457-464.</p></blockquote><p>根据 tf-idf 加权算法，可以得到 $\omega f(\omega_{j+k})$ 的表达式：</p><p>$$\omega f(\omega_{j+k}) = \frac{f(\omega,\omega’)}{n(\omega’)} = \frac{n(\omega,\omega’)}{n(\omega) * n(\omega’)}$$</p><p>其中 $n(\omega)$ 表示特征词汇 $\omega$ 在上下文中出现的数量，$n(\omega,\omega’)$ 表示上下文中 $\omega$ 与 $\omega’$ 共同出现的数量。</p><p>最终，某个特征词汇 $\omega_j$ 在滑动窗口上下文中的上下文向量表示为</p><p>$$ C_j = \sum^{n}_{i=1} c^i_j$$</p><h4 id="生成文本向量"><a href="#生成文本向量" class="headerlink" title="生成文本向量"></a>生成文本向量</h4><ul><li>计算文本集中所有特征词汇上下文向量的平均值</li></ul><p>$$\tau = \frac{\sum^n_{i=1}\sum^{z_i}_{j=1}Cj}{m}$$</p><p>其中 $z_i$ 表示文档 $d_i$ 中特征词汇总数，m 表示文 本集中所有不同特征词汇的总数量，n 表示文本集的文本总数量。</p><ul><li>生成文档 $d_i$ 的文本向量</li></ul><p>$$V_i =\frac{\sum^{z_i}_{j=1}C_j}{z_i} - \tau$$</p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>文章最后总结了一些经典数据集与实验应用 RI 算法之后准确率大多有所上升。论文 2 中最终总结了 RI 算法的优缺点。</p><h3 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h3><ul><li>计算量小</li><li>容易实现</li><li>处理效率高</li><li>潜在语义表现好，利用了上下文信息表示特征词的词向量，容易解决同义词、近义词等问题</li><li>降维性能好</li></ul><h3 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h3><ul><li>随机向量元素 (-1, +1, 0) 的随机性可能导致在计算特征词上下文向量时发生相加消减的情况，导致潜在语义信息丢失</li><li>论文 2 中选用的 tf-idf 计算出的加权值过小（此条仅对全文特征向量计算而言）</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>了解了 Random Indexing algorithm 的基本原理及应用。之后有精力希望能将 RI 算法的代码实现，并将其与其它词向量表示算法进行对比。</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/18/在-Ulysses-中使用-Latex-公式/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/18/在-Ulysses-中使用-Latex-公式/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">在 Ulysses 中使用 Latex 公式</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-18 17:27:58" itemprop="dateCreated datePublished" datetime="2017-08-18T17:27:58+08:00">2017-08-18</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Tool/" itemprop="url" rel="index"><span itemprop="name">Tool</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>在文档的头部加上如下代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~~ <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/x-mathjax-config"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="javascript">~~ MathJax.Hub.Config(&#123;<span class="attr">tex2jax</span>: &#123;<span class="attr">inlineMath</span>:[[<span class="string">'$latex'</span>,<span class="string">'$'</span>]]&#125;&#125;);</span></span><br><span class="line"><span class="undefined">~~ </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">~~ <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>即可在 Ulysses 中正常使用 Latex 公式啦。</p><p>输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~~ $latex a = frac&#123;1&#125;&#123;b&#125; $</span><br></pre></td></tr></table></figure><p>得到</p><p><img src="/images/pasted-182.png" alt="upload successful"></p><p>试试在 wp 里 latex 的显示：</p><p>~~ $latex a = frac{1}{b} $</p><p>除了多了两波浪号没别的问题，手动去掉吧。</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/14/机器之魂：聊天机器人是怎么工作的/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/14/机器之魂：聊天机器人是怎么工作的/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">机器之魂：聊天机器人是怎么工作的</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-14 15:52:47" itemprop="dateCreated datePublished" datetime="2017-08-14T15:52:47+08:00">2017-08-14</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>自早期的工业时代以来，人类就被能自主操作的设备迷住了。因为，它们代表了科技的“人化”。</p><p>而在今天，各种软件也在逐渐变得人性化。其中变化最明显的当属“聊天机器人”。</p><p>但是这些“机械”是如何运作的呢？首先，让我们回溯过去，探寻一种原始，但相似的技术。</p><h3 id="音乐盒是如何工作的"><a href="#音乐盒是如何工作的" class="headerlink" title="音乐盒是如何工作的"></a>音乐盒是如何工作的</h3><p><img src="/images/pasted-177.png" alt="upload successful"></p><p>早期自动化的样例 —— 机械音乐盒。<br>一组经过调音的金属齿排列成梳状结构，置于一个有针的圆柱边上。每根针都以一个特定的时间对应着一个音符。</p><p>当机械转动时，它便会在预定好的时间通过单个或者多个针的拨动来产生乐曲。如果要播放不同的歌，你得换不同的圆柱桶（假设不同的乐曲对应的特定音符是一样的）。</p><p>除了发出音符之外，圆筒的转动还可以附加一些其它的动作，例如移动小雕像等。不管怎样，这个音乐盒的基本机械结构是不会变的。</p><h3 id="聊天机器人是如何工作的"><a href="#聊天机器人是如何工作的" class="headerlink" title="聊天机器人是如何工作的"></a>聊天机器人是如何工作的</h3><p>输入的文本将经过一种名为“分类器”的函数处理，这种分类器会将一个输入的句子和一种“意图”（聊天的目的）联系起来，然后针对这种“意图”产生回应。</p><p><img src="/images/pasted-178.png" alt="upload successful"></p><p><a href="http://lauragelston.ghost.io/speakeasy/" target="_blank" rel="noopener">一个聊天机器人的例子</a></p><p>你可以将分类器看成是将一段数据（一句话）分入几个分类中的一种（即某种意图）的一种方式。输入一句话“how are you?”，将被分类成一种意图，然后将其与一种回应（例如“I’m good”或者更好的“I am well”）联系起来。</p><p>我们在基础科学中早学习了分类：黑猩猩属于“哺乳动物”类，蓝鸟属于“鸟”类，地球属于“行星”等等。</p><p>一般来说，文本分类有 3 种不同的方法。可以将它们看做是为了一些特定目的制造的软件机械，就如同音乐盒的圆筒一样。</p><h3 id="聊天机器人的文本分类方法"><a href="#聊天机器人的文本分类方法" class="headerlink" title="聊天机器人的文本分类方法"></a><strong>聊天机器人的文本分类方法</strong></h3><ul><li><strong>模式匹配</strong></li><li><strong>算法</strong></li><li><strong>神经网络</strong></li></ul><p>无论你使用哪种分类器，最终的结果一定是给出一个回应。音乐盒可以利用一些机械机构的联系来完成一些额外的“动作”，聊天机器人也如此。回应中可以使用一些额外的信息（例如天气、体育比赛比分、网络搜索等等），但是这些信息并不是聊天机器人的组成部分，它们仅仅是一些额外的代码。也可以根据句子中的某些特定“词性”来产生回应（例如某个专有名词）。此外，符合意图的回应也可以使用逻辑条件来判断对话的“状态”，以提供一些不同的回应，这也可以通过随机选择实现（好让对话更加“自然”）。</p><h3 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h3><p>早期的聊天机器人通过模式匹配来进行文本分类以及产生回应。这种方法常常被称为“暴力法”，因为系统的作者需要为某个回应详细描述所有模式。</p><p>这些模式的标准结构是“AIML”（人工智能标记语言）。这个名词里用了“人工智能”作为修饰词，但是<a href="https://medium.com/@gk_/the-ai-label-is-bullshit-559b171867ff" target="_blank" rel="noopener">它们完全不是一码事</a>。</p><p>下面是一个简单的模式匹配定义：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">aiml</span> <span class="attr">version</span> = <span class="string">"1.0.1"</span> <span class="attr">encoding</span> = <span class="string">"UTF-8"</span>?&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">category</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">pattern</span>&gt;</span> WHO IS ALBERT EINSTEIN <span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">template</span>&gt;</span>Albert Einstein was a German physicist.<span class="tag">&lt;/<span class="name">template</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">category</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">category</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">pattern</span>&gt;</span> WHO IS Isaac NEWTON <span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">template</span>&gt;</span>Isaac Newton was a English physicist and mathematician.<span class="tag">&lt;/<span class="name">template</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">category</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">category</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>DO YOU KNOW WHO * IS<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">template</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">srai</span>&gt;</span>WHO IS <span class="tag">&lt;<span class="name">star</span>/&gt;</span><span class="tag">&lt;/<span class="name">srai</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">template</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">category</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">aiml</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后机器经过处理会回答：</p><pre><code>Human: Do you know who Albert Einstein is
Robot: Albert Einstein was a German physicist.
</code></pre><p>它之所以知道别人问的是哪个物理学家，只是靠着与他或者她名字相关联的模式匹配。同样的，它靠着创作者预设的模式可以对任何意图进行回应。在给予它成千上万种模式之后，你终将能看到一个“类人”的聊天机器人出现。</p><p>2000 年的时候，John Denning 和他的同事就以这种方法做了个聊天机器人（<a href="http://mashable.com/2014/06/12/eugene-goostman-turing-test/" target="_blank" rel="noopener">相关新闻</a>），并通过了“图灵测试”。它设计的目标是模仿来自乌克兰的一个 13 岁的男孩，这孩子的英语水平很蹩脚。我在 2015 年的时候和 John 见过面，他没有矢口否认这个自动机的内部原理。因此，这个聊天机器人很可能就是用“暴力”的方法进行模式匹配。但它也证明了一点：在足够大的模式匹配定义的支持下，可以让大部分对话都贴近“自然”的程度。同时也符合了图灵（Alan Turing）的断言：制作用来糊弄人类的机器是“毫无意义”的。</p><p>使用这种方法做机器人的典型案例还有 <a href="http://www.pandorabots.com/" target="_blank" rel="noopener">PandoraBots</a>，他们宣称已经用他们的框架构建了超过 28.5 万个聊天机器人。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>暴力穷举法做自动机让人望而却步：对于每个输入都得有可用的模式来匹配其回应。人们由“老鼠洞”得到灵感，创建了模式的层级结构。</p><p>我们可以使用<strong>算法</strong>这种方法来减少分类器以便对机器进行管理，或者也可以说我们为它创建一个方程。这种方法是计算机科学家们称为“简化”的方法：问题需要<strong>缩减</strong>，那么解决问题的方法就是将其简化。</p><p>有一种叫做“朴素贝叶斯多项式模型”的经典文本分类算法，你可以在<a href="http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf" target="_blank" rel="noopener">这儿</a>或者别的地方学习它。下面是它的公式：</p><p><img src="/images/pasted-179.png" alt="upload successful"></p><p>实际用起它来比看上去要简单的多。给定一组句子，每个句子对应一个分类；接着输入一个新的句子，我们可以通过计算这个句子的单词在各个分类中的词频，找出各个分类的共性，并给每个分类一个<strong>分值</strong>（找出共性这点是很重要的：例如匹配到单词“cheese”（奶酪）比匹配到单词“it”要有意义的多）。最后，得到最高分值的分类很可能就是输入句子的同类。当然以上的说法是经过简化的，例如你还得先找到每个单词的<a href="https://en.wikipedia.org/wiki/Stemming" target="_blank" rel="noopener">词干</a>才行。不过，现在你应该对这种算法已经有了基本的概念。</p><p>下面是一个简单的训练集：</p><pre><code>class: weather
    &quot;is it nice outside?&quot;
    &quot;how is it outside?&quot;
    &quot;is the weather nice?&quot;

class: greeting
    &quot;how are you?&quot;
    &quot;hello there&quot;
    &quot;how is it going?&quot;
</code></pre><p>让我们来对几个简单的输入句子进行分类：</p><pre><code>input: &quot;Hi there&quot;
 term: &quot;hi&quot; (**no matches)**
 term: &quot;there&quot; **(class: greeting)**
 classification: **greeting **(score=1)

input: &quot;What’s it like outside?&quot;
 term: &quot;it&quot; **(class: weather (2), greeting)**
 term: &quot;outside **(class: weather (2) )**
 classification: **weather **(score=4)
</code></pre><p>请注意，“What’s it like outside”在分类时找到了另一个分类的单词，但是正确的分类给了单词较高的分值。通过算法公式，我们可以为句子计算匹配每个分类对应的词频，因此不需要去标明所有的模式。</p><p>这种分类器通过标定分类分值（计算词频）的方法给出最匹配语句的分类，但是它仍然有局限性。分值与概率不同，它仅仅能告诉我们句子的意图最有可能是哪个分类，而不能告诉我们它的所有匹配分类的可能性。因此，很难去给出一个阈值来判定是接受这个得分结果还是不接受这个结果。这种类型的算法给出的最高分仅仅能作为判断相关性的基础，它本质上作为分类器的效果还是比较差的。此外，这个算法不能接受 <em>is not</em> 类型的句子，因为它仅仅计算了 <em>it</em> 可能是什么。也就是说这种方法不适合做为包含 <em>not</em> 的否定句的分类。</p><p>有许多的聊天机器人框架<a href="https://medium.com/@gk_/text-classification-using-algorithms-e4d50dcba45#.ewnhttxa4" target="_blank" rel="noopener">都是用这种方法来判断意图分类</a>。而且大多数都是针对训练集进行词频计算，这种“幼稚”的方法有时还意外的有效。</p><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>人工神经网络发明于 20 世纪 40 年代，它通过迭代计算训练数据得到连接的加权值（“突触”），然后用于对输入数据进行分类。通过一次次使用训练数据计算改变加权值以使得神经网络的输出得到更高的“准确率”（低错误率）。</p><p><img src="/images/pasted-180.png" alt="upload successful"></p><p>上图为一种神经网络结构，其中包括神经元（圆）和突触（线）</p><p>其实除了当今的软件可以用更快的处理器、更大的内存外，这些结构并没有出现什么新奇的东西。当做数十万次的矩阵乘法（神经网络中的基本数学运算）的时候，运行内存和计算速度成为了关键问题。</p><p>在前面的方法里，每个分类都会给定一些例句。接着，根据词干进行分句，将所有单词作为神经网络的输入。然后遍历数据，进行成千上万次迭代计算，每次迭代都通过改变突触权重来得到更高的准确率。接着反过来通过对训练集输出值和神经网络计算结果的对比，对各层重新进行计算权重（反向传播）。这个“权重”可以类比成神经突触想记住某个东西的“力度”，你能记住某个东西是因为你曾多次见过它，在每次见到它的时候这个“权重”都会轻微地上升。</p><p>有时，在权重调整到某个程度后反而会使得结果逐渐变差，这种情况称为“过拟合”，在出现过拟合的情况下继续进行训练，反而会适得其反。</p><p><img src="/images/pasted-181.png" alt="upload successful"></p><p>训练好的神经网络模型的代码量其实很小，不过它需要一个很大的潜在权重矩阵。举个相对较小的样例，它的训练句子包括了 150 个单词、30 种分类，这可能产生一个 150x30 大小的矩阵；你可以想象一下，为了降低错误率，这么大的一个矩阵需要反复的进行 10 万次矩阵乘法。这也是为什么说需要高性能处理器的原因。</p><p>神经网络之所以能够做到既复杂又稀疏，归结于<a href="https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro" target="_blank" rel="noopener">矩阵乘法</a>和一种<a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener">缩小值至 -1，1 区间的公式</a>（即激活函数，这里指的是 Sigmoid），一个中学生也能在几小时内学会它。其实真正困难的工作是清洗训练数据。</p><p>就像前面的模式匹配和算法匹配一样，神经网络也有各种各样的变体，有一些变体会十分复杂。不过它的基本原理是相同的，做的主要工作也都是进行分类。</p><p>机械音乐盒并不了解乐理，同样的，<strong>聊天机器人并不了解语言</strong>。</p><p>聊天机器人实质上就是寻找短语集合中的模式，每个短语还能再分割成单个单词。在聊天机器人内部，除了它们存在的模式以及训练数据之外的<strong>单词其实并没有意义</strong>。为这样的“机器人”贴上“人工智能”的标签其实<a href="https://medium.com/@gk_/the-ai-label-is-bullshit-559b171867ff#.3tlhftemt" target="_blank" rel="noopener">也很糟糕</a>。</p><p>总结：聊天机器人就像机械音乐盒一样：它就是<strong>一个根据模式来进行输出的机器</strong>，只不过它不用圆筒和针，而是使用软件代码和数学原理。</p><blockquote><p>发布于掘金 <a href="https://juejin.im/post/599155d86fb9a03c467c151d" target="_blank" rel="noopener">https://juejin.im/post/599155d86fb9a03c467c151d</a></p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/12/解决将-ionic2-升级至-3-时出现的-webpackJsonp-问题/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/12/解决将-ionic2-升级至-3-时出现的-webpackJsonp-问题/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">解决将 ionic2 升级至 3 时出现的 webpackJsonp 问题</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-12 18:28:26" itemprop="dateCreated datePublished" datetime="2017-08-12T18:28:26+08:00">2017-08-12</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Front-End/" itemprop="url" rel="index"><span itemprop="name">Front End</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>ionic3 修复了2.x 存在的 ion-select 组件的 interface 等 bug，因此对其进行升级。修改 package.json，删除 node_module 目录，在 npm I 的时候依次按照提示在 package.json 中将不符合版本的库改为兼容版本。</p><p>升级完成之后 build 时提示 webpackjsonp is not defined，翻阅 README 的 Change log 发现新版 cli 脚手架写的 webpack 配置有所改变，将公用部分使用 CommonsChunkPlugin 额外打了一个包，此包命名为 vendor。在 app 入口文件中引用该公用包解决问题。</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/09/卷积神经网络/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/09/卷积神经网络/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">卷积神经网络</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-09 16:25:34" itemprop="dateCreated datePublished" datetime="2017-08-09T16:25:34+08:00">2017-08-09</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CNN-是怎么学习的？学习了什么？"><a href="#CNN-是怎么学习的？学习了什么？" class="headerlink" title="CNN 是怎么学习的？学习了什么？"></a>CNN 是怎么学习的？学习了什么？</h2><p><strong>这篇文章是深度学习系列的一部分。你可以在</strong><a href="https://github.com/xitu/gold-miner/blob/master/TODO/deep-learning-1-setting-up-aws-image-recognition.md" target="_blank" rel="noopener"><strong>这里</strong></a><strong>查看第一部分，以及在</strong><a href="https://github.com/xitu/gold-miner/blob/master/TODO/deep-learning-3-more-on-cnns-handling-overfitting.md" target="_blank" rel="noopener"><strong>这里</strong></a><strong>查看第三部分。</strong></p><p><img src="/images/pasted-171.png" alt="upload successful"></p><p>这一周，我们将探索卷积神经网络（CNN）的内部工作原理。你可能会问：在网络内部究竟发生了什么？它们是怎样学习的？</p><p>这门课程遵循自上而下的学习方法与理念。因此一般来说，我们在开始学习的时候就能立即玩到所有的模型，然后我们会逐渐深入其内部的工作原理。因此，本系列也将会逐渐深入探索神经网络的内部工作原理。现在仅仅是第二周，让我们朝着最终的目标迈进吧！</p><p>在上周，我在猫狗图像集上训练了 Vgg 16 模型。我想先聊一下为什么说使用预先训练好的模型是一种很好的方法。为了使用这些模型，首先你得要弄清楚这些模型到底学习的是什么。从本质上说，CNN 学习的是过滤器，并将学习到的过滤器应用于图像。当然，这些“过滤器”和你在 Instagram 里用的滤镜（英文也为“filter”）并不是一种东西，但它们其实有一些相同之处。CNN 会使用一个小方块遍历整张图片，通常将这个小方块称为“窗口”。接下来，网络会在图片中查找与过滤器匹配的图片内容。在第一层，网络可能只学习到了一些简单的事物（例如对角线）。在之后的每一层中，网络都将结合前面找到的特征，持续学习更加复杂的概念。单单听这些概念可能会让人比较迷糊，让我们直接来看一些例子。<a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="noopener">Zeiler and Fergus (2013)</a> 为可视化 CNN 学习过程做出了一项很棒的工作。下图是他们在论文中用的 CNN 模型，赢得 Imagenet 竞赛的 Vgg16 模型就是基于这个模型做出来的。</p><p><img src="/images/pasted-172.png" alt="upload successful"></p><p>CNN，作者：Zeiler &amp; Fergus (2013)</p><p>可能你现在会觉得这个图片很难懂，请不要慌！让我们先从我们可以在图中看到的东西说起吧。首先，输入图像是正方形，大小为 224x224 像素。我之前说的过滤器大小是 7x7 像素大小。该模型有一个输入层，7 个隐藏层以及一个输出层。输出层的“C”指的是模型的预测分类数量。现在让我们来了解 CNN 中最有趣的部分：这个神经网络在每一层中都学到了什么！</p><p><img src="/images/pasted-173.png" alt="upload successful"></p><p>上图为 CNN 的第二层。左边的图像代表了 CNN 的这层网络在右边的真实图片中学习到的内容。<br>在 CNN 的第二层中，你可以发现这个模型已经不仅仅是去提取对角线了，它找到了一些更有意思的形状特征。例如在第二排第二列的方块中，你可以看到模型正在提取圆形；还有，最后一个方块表明模型正在专注于识别图中的一个直角作为特征。</p><p><img src="/images/pasted-174.png" alt="upload successful"></p><p>上图为 CNN 的第三层。<br>在第三层中，我们可以看到模型已经开始学习一些更具体的东西。第一个方块中的图像表明模型已经能够识别出一些地理特征；第二排第二列的方块表明模型正在识别车轮；倒数第二个方块表明模型正在识别人类。</p><p><img src="/images/pasted-175.png" alt="upload successful"></p><p>CNN 的第四层与第五层</p><p>在最后，第四层与第五层保持前面模型越来越具体的趋势。第五层找到了对解决我们的猫狗问题非常有帮助的特征。与此同时，它还识别出了独轮车，以及鸟类、爬行动物的眼睛。请注意，这些图像仅仅展示了每一层学习到的东西的极小一部分。</p><p>希望上面的文字已经告诉了你为什么使用预先训练好的模型是很有用的。如果你想更多的了解这块领域的研究，你可以搜索“迁移学习”（transfer learning）的相关内容。虽然我们的猫狗问题训练集仅仅只有 25000 张图片，一个新的模型可能还无法从这些图片中学习到所有的特征，但我们的 Vgg16 模型已经相当“了解”怎么去识别猫和狗了。最后，通过“微调”（Finetuning） Vgg16 模型的最后一层，让其不再输出 1000 多种分类的概率，而是直接输出二分类 —— 猫和狗。</p><p>如果你对深度学习背后的数学知识感兴趣，<a href="http://cs231n.github.io/" target="_blank" rel="noopener">Stanford’s CNN pages</a> 是很好的参考材料。他们首次以“数学之美”解释了浅层神经网络。</p><hr><h4 id="微调及线性层（全连接层）"><a href="#微调及线性层（全连接层）" class="headerlink" title="微调及线性层（全连接层）"></a>微调及线性层（全连接层）</h4><p>上周，我用这个预先训练好的 Vgg16 模型不能很自然的区分猫和狗这两个分类下的图片，而是提出了 1000 余种分类。此外，这个模型并不会直接输出“猫”和“狗”的分类，而是输出猫和狗的一些特定品种。那我们如何修改这个模型，让它能够有效地对猫和狗进行分类呢？</p><p>有种可选方案：手动将这些品种分到猫和狗中去，然后计算其概率之和。但是，这种做法会丢弃一些关键信息。例如，如果图片中只有一根骨头，但它很可能是一张属于狗的照片。如果我们仅查看这些品种分为猫狗的概率，前面提到的这种信息很可能会丢失。因此在模型的最后，我们加入一个线性层（全连接层），它将仅输出两种分类。实际上，Vgg16 模型的最后有 3 层全连接层。我们可以微调这些层，通过反向传播来训练它们。反向传播算法常常被人看成是一种抽象的魔法，但其实它只是简单应用链式求导法则。你可以暂时忽略这些数学上的细节，TensorFlow、Theano 和其它深度学习库已经帮你做好了这些工作。</p><p>如果你正在运行 Fast AI 课程 lesson 2 的 notebook，我建议你最好先只使用 notebook 的样例图片。如果你运行 p2 的实例，可能会由于保存、加载 numpy 数组将内存耗尽。</p><hr><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>前面我们讨论了网络最后的线性层（全连接层）。然而，神经网络的所有层都不是线性的。在神经网络计算出每个神经元的参数之后，我们需要将它们的计算结果作为参数输入到激活函数中。人工神经网络基本上由矩阵乘法组成，如果我们只使用线性计算的话，我们只能将它们一个个叠加在一起，并不能做成一个很深的网络。因此，我们会经常在网络的各层使用非线性的激活函数。通过将重重线性与非线性函数叠加在一起，理论上我们可以对任何事物进行建模。下面是三种最受欢迎的非线性激活函数：</p><ul><li>Sigmoid <strong>（将值转换到 0，1 间）</strong></li><li>TanH <strong>（将值转换到 -1，1 间）</strong></li><li>ReLu <strong>（如果值为负则输出 0，否则输出原值）</strong></li></ul><p><img src="/images/pasted-176.png" alt="upload successful"></p><p>上图为最常用的激活函数：Sigmoid、Tanh 和 ReLu（又名修正线性单元）<br>目前，ReLu 是使用的最多的非线性激活函数，主要原因是它可以减少梯度消失的可能性，以及保持稀疏特征。稍后会讨论这方面的更多详情。因为我们希望模型最后能够输出确定的内容，因此模型的最后一层通常使用一种另外的激活函数 —— softmax。softmax 函数是一种非常受欢迎的分类器。</p><p>在微调完 Vgg16 模型的最后一层之后，它总共有 138357544 个参数。谢天谢地，我们不需要手动计算各种梯度 XD。下一周我们将更深入地了解 CNN 的工作原理，讨论主题为欠拟合和过拟合。</p><p>如果你喜欢这篇文章，请将它推荐给其他人吧！你也可以关注此系列文章，跟上 Fast AI 课程的进度。下篇文章再会！</p><blockquote><p>发布于掘金 <a href="https://juejin.im/post/598ac6a55188257dd366367f" target="_blank" rel="noopener">https://juejin.im/post/598ac6a55188257dd366367f</a></p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/08/09/如何将时间序列问题用-Python-转换成为监督学习问题/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/08/09/如何将时间序列问题用-Python-转换成为监督学习问题/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">如何将时间序列问题用 Python 转换成为监督学习问题</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-08-09 16:23:01" itemprop="dateCreated datePublished" datetime="2017-08-09T16:23:01+08:00">2017-08-09</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>一些机器学习方法（例如深度学习）可以用于进行时间序列预测。</p><p>在使用这些机器学习方法前，必须先将时间序列预测问题转化为监督学习问题。也就是说，需要将一个时间序列转换成一组包含成对输入输出的序列。</p><p>在这篇教程里，你将了解如何将单变量时间序列预测问题和多变量时间序列预测问题转换成监督学习问题，以使用机器学习算法。</p><p>读完这篇教程，你将会了解：</p><ul><li>如何编写一个将时间序列数据集转换为监督学习数据集的函数。</li><li>如何转换一元时间序列数据以使用机器学习。</li><li>如何转换多元时间序列数据以使用机器学习。</li></ul><p>让我们开始吧。</p><p><img src="/images/pasted-170.png" alt="upload successful"></p><p>题图：如何将时间序列问题用 Python 转换成为监督学习问题</p><p><a href="https://www.flickr.com/photos/quimgil/8490510169/" target="_blank" rel="noopener">Quim Gil</a> 拍摄，版权所有。</p><h2 id="时间序列-vs-监督学习"><a href="#时间序列-vs-监督学习" class="headerlink" title="时间序列 vs 监督学习"></a>时间序列 vs 监督学习</h2><p>在正式开始之前，让我们先花点时间更好地了解一下时间序列和监督学习的数据集结构。</p><p>单个时间序列由一系列按照时间排序的数字序列组成。可以将其理解为一列有序值。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><p>而一个监督学习问题是由一组输入（<em>X</em>）和一组输出（<em>y</em>）组成，算法可以学会如何通过输入值来预测输出值。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X,  y</span><br><span class="line">1 2</span><br><span class="line">2,  3</span><br><span class="line">3,  4</span><br><span class="line">4,  5</span><br><span class="line">5,  6</span><br><span class="line">6,  7</span><br><span class="line">7,  8</span><br><span class="line">8,  9</span><br></pre></td></tr></table></figure><p>可以参阅这篇文章，学习更多有关知识：</p><ul><li><a href="http://machinelearningmastery.com/time-series-forecasting-supervised-learning/" target="_blank" rel="noopener">Time Series Forecasting as Supervised Learning</a></li></ul><h2 id="Pandas-的-shift-函数"><a href="#Pandas-的-shift-函数" class="headerlink" title="Pandas 的 shift() 函数"></a>Pandas 的 shift() 函数</h2><p>我们将时间序列数据转化为监督学习问题的关键就是使用 Pandas 的 <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html" target="_blank" rel="noopener">shift()</a> 函数。</p><p>给定一个 DataFrame，<em>shift()</em> 函数会将输入的列复制一份，然后将副本列整体往后移动（最前面的数据空位会用 NaN 填充）或者往前移动（最后面的数据空位会用 NaN 填充）。</p><p>这样可以创建一个滞后值列，加上观察列，就能将时间序列数据集变成监督学习数据集的格式。</p><p>让我们看看 shift 函数实际用起来效果如何。</p><p>我们可以通过下面的代码模拟一个长度为 10 的时间序列数据集，此时它在 DataFrame 中为单独的一列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">df = DataFrame()</span><br><span class="line">df[<span class="string">'t'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><p>运行上面的样例，将时间序列数据输出，其每一行都为带有索引的观察组数据。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   t</span><br><span class="line">0  0</span><br><span class="line">1  1</span><br><span class="line">2  2</span><br><span class="line">3  3</span><br><span class="line">4  4</span><br><span class="line">5  5</span><br><span class="line">6  6</span><br><span class="line">7  7</span><br><span class="line">8  8</span><br><span class="line">9  9</span><br></pre></td></tr></table></figure><p></p><p>我们可以在数据顶部插入一行，将观察组的数据整体下挪一位。由于最上面插入的新行没有数据，因此我们可以用 NaN 填充来表示这儿“没有数据”。</p><p>shift 函数可以完成这些操作。我们可以将 shift 函数“挪动”过的新列插入原始序列的旁边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">df = DataFrame()</span><br><span class="line">df[<span class="string">'t'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">df[<span class="string">'t-1'</span>] = df[<span class="string">'t'</span>].shift(<span class="number">1</span>)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><p>运行上面的样例，你将得到一个包含两列的数据集。第一列是原始的观察组，第二列是经由 shift 函数挪动生成的新列。</p><p>可以看到，经过将序列移动一次的操作之后，我们得到了一个原始的监督学习问题（虽然此时的 <em>X</em> 和 <em>y</em> 的排序明显是错的）。忽略最前面的表头，第一行存在 NaN 值，因此需要将其丢弃。在第二行，我们可以将第二列的 0.0 作为输入值（也就是 <em>X</em>），将第一列的 1 作为输出值（或 <em>y</em>）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   t  t-1</span><br><span class="line">0  0  NaN</span><br><span class="line">1  1  0.0</span><br><span class="line">2  2  1.0</span><br><span class="line">3  3  2.0</span><br><span class="line">4  4  3.0</span><br><span class="line">5  5  4.0</span><br><span class="line">6  6  5.0</span><br><span class="line">7  7  6.0</span><br><span class="line">8  8  7.0</span><br><span class="line">9  9  8.0</span><br></pre></td></tr></table></figure><p>如果我们重复 shift 步骤，让原始列挪动 2 位、3 位或者更多位，我们就能得到一系列的输入数据（<em>X</em>），由这些输入值就能去预测输出值（<em>y</em>）了。</p><p>shift 操作能也能接受负整数作为参数。如果你这么做，它会在列底部插入新行，从而使得原列向上移动。下面是例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line">df = DataFrame()</span><br><span class="line">df[<span class="string">'t'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">df[<span class="string">'t+1'</span>] = df[<span class="string">'t'</span>].shift(<span class="number">-1</span>)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><p>运行上面的样例，可以看到新列中的最后一个值为 NaN。</p><p>此时可以将预测列作为输入值（<em>X</em>），将第二列作为输出值（<em>y</em>）。也就是给定输入值 0 可以用于预测输出值 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   t  t+1</span><br><span class="line">0  0  1.0</span><br><span class="line">1  1  2.0</span><br><span class="line">2  2  3.0</span><br><span class="line">3  3  4.0</span><br><span class="line">4  4  5.0</span><br><span class="line">5  5  6.0</span><br><span class="line">6  6  7.0</span><br><span class="line">7  7  8.0</span><br><span class="line">8  8  9.0</span><br><span class="line">9  9  NaN</span><br></pre></td></tr></table></figure><p>从技术上说，在时间序列预测问题的术语中，当前时间（<em>t</em>）和未来时间（<em>t+1, t+n</em>）为待预测时间，过去时间（<em>t-1, t-n</em>）则用于预测。</p><p>从上面的例子中，我们可以学会如何使用通过 shift 函数正向或反向移动序列，生成新的 DataFrame，将时间序列问题转变成监督学习问题的输入-输出模式。</p><p>这不仅可以解决经典的 <em>X -> y</em> 类预测问题，也可以用于输入输出值都是序列的 <em>X -> Y</em> 类预测。</p><p>另外，shift 函数也能用于多元时间序列问题中。这类问题中包含多列观察组（例如温度、气压等）。时间序列中的所有变量都能用通过向前或向后挪动，生成多元输入值与输出值序列。稍后我们将探讨这类问题。</p><h2 id="series-to-supervised-函数"><a href="#series-to-supervised-函数" class="headerlink" title="series_to_supervised() 函数"></a>series_to_supervised() 函数</h2><p>我们可以使用 Pandas 的 <em>shift()</em> 函数，在给定希望得到的输入值、输出值序列长度后自动生成时间序列问题的新格式数据。</p><p>这是个很有用的工具。我们可以通过机器学习算法研究各种时间序列问题格式，探究哪种格式能够得到效果更佳的模型。</p><p>在本节中，我们将创建一个新的 Python 函数，名为 <em>series_to_supervised()</em>。它可以将多元时间序列问题与一元时间序列问题转换为监督学习数据集的格式。</p><p>这个函数接收以下 4 个参数：</p><ul><li><strong>data</strong>：必填，待转换的序列，数据类型为 list 或 2 维 NumPy array。</li><li><strong>n_in</strong>： 可选，滞后组（作为输入值 X）的数量。范围可以在 [1..len(data)] 之间，默认值为 1。</li><li><strong>n_out</strong>： 可选，观察组（作为输出值 y）的数量。范围可以在 [0..len(data)-1] 之间，默认值为 1。</li><li><strong>dropnan</strong>：选填，决定是否抛去包含 NaN 的行。类型为 Boolean，默认值为 True。</li></ul><p>函数将会返回一个值：</p><ul><li><strong>return</strong>：返回监督学习格式的数据集，数据类型为 Pandas DataFrame。</li></ul><p>新数据集 DataFrame 格式，每一列都由原变量名称和移动步数命名，让你可以根据给定的一元或多元时间序列问题设计出各种移动步数的序列。</p><p>在 DataFrame 返回时，你可以对其行进行分割，根据你的需要决定如何将返回的 DataFrame 分成 X 和 y 两部分。</p><p>这个函数的参数都设置了默认值，因此可以直接调用它处理你的数据，这种默认情况它将会返回一个 <em>t-1</em> 作为 X，<em>t</em> 作为 y 的 DataFrame。</p><p>这个函数已确定同时兼容 Python2 和 Python3。</p><p>下面为完整代码，并写好了注释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br></pre></td></tr></table></figure><p>你觉得可以怎样提高这个函数的鲁棒性或者可读性吗？请留言在评论区。</p><p>至此我们已经得到了整个函数，接下来探索它的用法。</p><h2 id="单步或单变量预测"><a href="#单步或单变量预测" class="headerlink" title="单步或单变量预测"></a>单步或单变量预测</h2><p>在时间序列预测问题中通常使用滞后时间（例如 t-1）作为输入变量来预测当前时间（t）。</p><p>这种问题被称为单步预测。</p><p>下面展示了使用滞后一个时间步的时间（t-1）来预测当前时间（t）的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br><span class="line">  </span><br><span class="line">  values = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">  data = series_to_supervised(values)</span><br><span class="line">  print(data)</span><br></pre></td></tr></table></figure><p>运行样例，输出转换后的时间序列。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   var1(t-1)  var1(t)</span><br><span class="line">1        0.0        1</span><br><span class="line">2        1.0        2</span><br><span class="line">3        2.0        3</span><br><span class="line">4        3.0        4</span><br><span class="line">5        4.0        5</span><br><span class="line">6        5.0        6</span><br><span class="line">7        6.0        7</span><br><span class="line">8        7.0        8</span><br><span class="line">9        8.0        9</span><br></pre></td></tr></table></figure><p>可以看到，观察组被命名为“<em>var1</em>”，作为输入值的观察组被命名为（<em>t-1</em>），输出值组被命名为（<em>t</em>）。</p><p>此外，可以看到包含 NaN 的行已经被自动从 DataFrame 中移除。</p><p>我们可以任意给定输入序列数量的值来重复运行这个例子。例如输入 3，我们事先已经将输入序列的数量定义为了一个参数。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = series_to_supervised(values, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>完整样例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">values = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">data = series_to_supervised(values, <span class="number">3</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>再次运行样例，输出重新构造的序列，可以看到输入序列准确无误地从左至右裴烈，作为预测项的输入值在最右边。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   var1(t-3)  var1(t-2)  var1(t-1)  var1(t)</span><br><span class="line">3        0.0        1.0        2.0        3</span><br><span class="line">4        1.0        2.0        3.0        4</span><br><span class="line">5        2.0        3.0        4.0        5</span><br><span class="line">6        3.0        4.0        5.0        6</span><br><span class="line">7        4.0        5.0        6.0        7</span><br><span class="line">8        5.0        6.0        7.0        8</span><br><span class="line">9        6.0        7.0        8.0        9</span><br></pre></td></tr></table></figure><h2 id="多步或序列预测"><a href="#多步或序列预测" class="headerlink" title="多步或序列预测"></a>多步或序列预测</h2><p>还有一类预测问题：使用过去的观察组来对未来的观察组序列做预测。</p><p>可以将这类问题成为序列预测问题或者多步预测问题。</p><p>我们可以通过规定另一个参数来将序列预测问题的时间序列重新构造。例如，我们可以把 2 个过去的观察组转变为 2 个未来的观察组，从而重新构造预测问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=series_to_supervised(values,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>完整样例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br><span class="line"></span><br><span class="line">values = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">data = series_to_supervised(values, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行样例，可以看到将（<em>t-n</em>）作为输入变量、将（<em>t+n</em>）作为输出变量时，与将当前观察组（<em>t</em>）作为输出的不同之处。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   var1(t-2)  var1(t-1)  var1(t)  var1(t+1)</span><br><span class="line">2        0.0        1.0        2        3.0</span><br><span class="line">3        1.0        2.0        3        4.0</span><br><span class="line">4        2.0        3.0        4        5.0</span><br><span class="line">5        3.0        4.0        5        6.0</span><br><span class="line">6        4.0        5.0        6        7.0</span><br><span class="line">7        5.0        6.0        7        8.0</span><br><span class="line">8        6.0        7.0        8        9.0</span><br></pre></td></tr></table></figure><h2 id="多元预测"><a href="#多元预测" class="headerlink" title="多元预测"></a>多元预测</h2><p>还有一种重要的时间序列类型，叫做多元时间序列。</p><p>这种情况我们会将多个不同的指标作为观察组，并预测它们中的一个或多个的值。</p><p>例如，我们有两组时间序列观察组 obs1 和 obs2，希望预测它们或它们中的一者。</p><p>我们同样可以调用 <em>series_to_supervised()</em>。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">raw = DataFrame()</span><br><span class="line">raw[<span class="string">'ob1'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">raw[<span class="string">'ob2'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">50</span>, <span class="number">60</span>)]</span><br><span class="line">values = raw.values</span><br><span class="line">data = series_to_supervised(values)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行样例，将会得到经过重新构造后的数据。数据显示了分别处于同一个时间的两组变量作为输入组以及输出组。</p><p>与之前一样，根据问题的需要，可以将列分入 <em>X</em> 和 <em>y</em> 两个子集中，需要注意的是如果放入了 <em>var1</em> 做为观察组，那就要放入 <em>var2</em> 作为待预测组。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   var1(t-1)  var2(t-1)  var1(t)  var2(t)</span><br><span class="line">1        0.0       50.0        1       51</span><br><span class="line">2        1.0       51.0        2       52</span><br><span class="line">3        2.0       52.0        3       53</span><br><span class="line">4        3.0       53.0        4       54</span><br><span class="line">5        4.0       54.0        5       55</span><br><span class="line">6        5.0       55.0        6       56</span><br><span class="line">7        6.0       56.0        7       57</span><br><span class="line">8        7.0       57.0        8       58</span><br><span class="line">9        8.0       58.0        9       59</span><br></pre></td></tr></table></figure><p>可以看到，通过上面这样给定输入序列和输出序列的数量生成的新的序列，可以帮助你轻松地完成多元时间序列的预测。</p><p>例如，下面将把 1 作为输入列数量，将 2 作为输出列（预测列）数量，重新构造预测序列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> concat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">series_to_supervised</span><span class="params">(data, n_in=<span class="number">1</span>, n_out=<span class="number">1</span>, dropnan=True)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数用途：将时间序列转化为监督学习数据集。</span></span><br><span class="line"><span class="string">  参数说明：</span></span><br><span class="line"><span class="string">    data: 观察值序列，数据类型可以是 list 或者 NumPy array。</span></span><br><span class="line"><span class="string">    n_in: 作为输入值(X)的滞后组的数量。</span></span><br><span class="line"><span class="string">    n_out: 作为输出值(y)的观察组的数量。</span></span><br><span class="line"><span class="string">    dropnan: Boolean 值，确定是否将包含 NaN 的行移除。</span></span><br><span class="line"><span class="string">  返回值:</span></span><br><span class="line"><span class="string">    经过转换的用于监督学习的 Pandas DataFrame 序列。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  n_vars = <span class="number">1</span> <span class="keyword">if</span> type(data) <span class="keyword">is</span> list <span class="keyword">else</span> data.shape[<span class="number">1</span>]</span><br><span class="line">  df = DataFrame(data)</span><br><span class="line">  cols, names = list(), list()</span><br><span class="line">  <span class="comment"># 输入序列 (t-n, ... t-1)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(n_in, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">    cols.append(df.shift(i))</span><br><span class="line">    names += [(<span class="string">'var%d(t-%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 预测序列 (t, t+1, ... t+n)</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n_out):</span><br><span class="line">    cols.append(df.shift(-i))</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t)'</span> % (j+<span class="number">1</span>)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      names += [(<span class="string">'var%d(t+%d)'</span> % (j+<span class="number">1</span>, i)) <span class="keyword">for</span> j <span class="keyword">in</span> range(n_vars)]</span><br><span class="line">  <span class="comment"># 将所有列拼合</span></span><br><span class="line">  agg = concat(cols, axis=<span class="number">1</span>)</span><br><span class="line">  agg.columns = names</span><br><span class="line">  <span class="comment"># drop 掉包含 NaN 的行</span></span><br><span class="line">  <span class="keyword">if</span> dropnan:</span><br><span class="line">    agg.dropna(inplace=<span class="keyword">True</span>)</span><br><span class="line">  <span class="keyword">return</span> agg</span><br><span class="line"></span><br><span class="line">raw = DataFrame()</span><br><span class="line">raw[<span class="string">'ob1'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">raw[<span class="string">'ob2'</span>] = [x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">50</span>, <span class="number">60</span>)]</span><br><span class="line">values = raw.values</span><br><span class="line">data = series_to_supervised(values, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>运行样例，将会展示重新构造的很大的 DataFrame。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">   var1(t-1)  var2(t-1)  var1(t)  var2(t)  var1(t+1)  var2(t+1)</span><br><span class="line">1        0.0       50.0        1       51        2.0       52.0</span><br><span class="line">2        1.0       51.0        2       52        3.0       53.0</span><br><span class="line">3        2.0       52.0        3       53        4.0       54.0</span><br><span class="line">4        3.0       53.0        4       54        5.0       55.0</span><br><span class="line">5        4.0       54.0        5       55        6.0       56.0</span><br><span class="line">6        5.0       55.0        6       56        7.0       57.0</span><br><span class="line">7        6.0       56.0        7       57        8.0       58.0</span><br><span class="line">8        7.0       57.0        8       58        9.0       59.0</span><br></pre></td></tr></table></figure><p>你可以用你自己的数据集多做几次实验，来试试哪种重构的效果更好。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇教程中，你已经了解了如何使用 Python 将时间序列数据集转换为监督学习问题。</p><p>特别的，你了解了：</p><ul><li>有关 Pandas <em>shift()</em> 函数的知识，以及它如何自动将时间序列数据转化为监督学习数据集。</li><li>如何将一元时间序列重构成单步或多步监督学习问题。</li><li>如何将多元时间序列重构成单步或多步监督学习问题。</li></ul></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/07/21/使用-Python-spaCy-进行简易自然语言处理/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/07/21/使用-Python-spaCy-进行简易自然语言处理/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">使用 Python+spaCy 进行简易自然语言处理</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-07-21 22:35:00" itemprop="dateCreated datePublished" datetime="2017-07-21T22:35:00+08:00">2017-07-21</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>自然语言处理（NLP）是人工智能领域最重要的部分之一。它在许多智能应用中担任了关键的角色，例如聊天机器人、正文提取、多语翻译以及观点识别等应用。业界 NLP 相关的公司都意识到了，处理非结构文本数据时，不仅要看正确率，还需要注意是否能快速得到想要的结果。</p><p>NLP 是一个很宽泛的领域，它包括了文本分类、实体识别、机器翻译、问答系统、概念识别等子领域。在我最近的一篇<a href="https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/" target="_blank" rel="noopener">文章</a>中，我探讨了许多用于实现 NLP 的工具与组件。在那篇文章中，我更多的是在描述<a href="http://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>（Natural Language Toolkit）这个伟大的库。</p><p>在这篇文章中，我会将 spaCy —— 这个现在最强大、最先进的 NLP python 库分享给你们。</p><hr><h2 id="内容提要"><a href="#内容提要" class="headerlink" title="内容提要"></a>内容提要</h2><ol><li>spaCy 简介及安装方法</li><li><p>spaCy 的管道与属性</p><ul><li>Tokenization</li><li>词性标注</li><li>实体识别</li><li>依存句法分析</li><li>名词短语</li></ul></li><li><p>集成词向量计算</p></li><li>使用 spaCy 进行机器学习</li><li>与 NLTK 和 CoreNLP 对比</li></ol><hr><h2 id="1-spaCy-简介及安装方法"><a href="#1-spaCy-简介及安装方法" class="headerlink" title="1. spaCy 简介及安装方法"></a>1. spaCy 简介及安装方法</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>spaCy 由 cython（Python 的 C 语言拓展，旨在让 python 程序达到如同 C 程序一样的性能）编写，因此它的运行效率非常高。spaCy 提供了一系列简洁的 API 方便用户使用，并基于已经训练好的机器学习与深度学习模型实现底层。</p><hr><h3 id="1-2-安装"><a href="#1-2-安装" class="headerlink" title="1.2 安装"></a>1.2 安装</h3><p>spaCy 及其数据和模型可以通过 pip 和安装工具轻松地完成安装。使用下面的命令在电脑中安装 spaCy：</p><pre><code>sudo pip install spacy
</code></pre><p>如果你使用的是 Python3，请用 “pip3” 代替 “pip”。</p><p>或者你也可以在 <a href="https://pypi.python.org/pypi/spacy" target="_blank" rel="noopener">这儿</a> 下载源码，解压后运行下面的命令安装：</p><pre><code>python setup.py install
</code></pre><p>在安装好 spacy 之后，请运行下面的命令以下载所有的数据集和模型：</p><pre><code>python -m spacy.en.download all
</code></pre><p>一切就绪，现在你可以自由探索、使用 spacy 了。</p><h2 id="2-spaCy-的管道（Pipeline）与属性（Properties）"><a href="#2-spaCy-的管道（Pipeline）与属性（Properties）" class="headerlink" title="2. spaCy 的管道（Pipeline）与属性（Properties）"></a>2. spaCy 的管道（Pipeline）与属性（Properties）</h2><p>spaCy 的使用，以及其各种属性，是通过创建管道实现的。在加载模型的时候，spaCy 会将管道创建好。在 spaCy 包中，提供了各种各样的<a href="https://github.com/explosion/spacy-models/" target="_blank" rel="noopener">模块</a>，这些模块中包含了各种关于词汇、训练向量、语法和实体等用于语言处理的信息。</p><p>下面，我们会加载默认的模块（english-core-web 模块）。</p><pre><code>import spacy
nlp = spacy.load(“en”)
</code></pre><p>“nlp” 对象用于创建 document、获得 linguistic annotation 及其它的 nlp 属性。首先我们要创建一个 document，将文本数据加载进管道中。我使用了来自猫途鹰网的旅店评论数据。这个数据文件可以在<a href="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/04/04080929/Tripadvisor_hotelreviews_Shivambansal.txt" target="_blank" rel="noopener">这儿</a>下载。</p><pre><code>document = unicode(open(filename).read().decode(&apos;utf8&apos;))
document = nlp(document)
</code></pre><p>这个 document 现在是 spacy.english 模型的一个 class，并关联上了许多的属性。可以使用下面的命令列出所有 document（或 token）的属性：</p><pre><code>dir(document)
&gt;&gt; [ &apos;doc&apos;, &apos;ents&apos;, … &apos;mem&apos;]
</code></pre><p>它会输出 document 中各种各样的属性，例如：token、token 的 index、词性标注、实体、向量、情感、单词等。下面让我们会对其中的一些属性进行一番探究。</p><h3 id="2-1-Tokenization"><a href="#2-1-Tokenization" class="headerlink" title="2.1 Tokenization"></a>2.1 Tokenization</h3><p>spaCy 的 document 可以在 tokenized 过程中被分割成单句，这些单句还可以进一步分割成单词。你可以通过遍历文档来读取这些单词：</p><pre><code># document 的首个单词
document[0]
&gt;&gt; Nice

# document 的最后一个单词  
document[len(document)-5]
&gt;&gt; boston

# 列出 document 中的句子
list(document.sents)
&gt;&gt; [ Nice place Better than some reviews give it credit for.,
 Overall, the rooms were a bit small but nice.,
...
Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).]
</code></pre><h3 id="2-2-词性标注-POS-Tag"><a href="#2-2-词性标注-POS-Tag" class="headerlink" title="2.2 词性标注(POS Tag)"></a>2.2 词性标注(POS Tag)</h3><p>词性标注即标注语法正确的句子中的词语的词性。这些标注可以用于信息过滤、统计模型，或者基于某些规则进行文本解析。</p><p>来看看我们的 document 中所有的词性标注：</p><pre><code># 获得所有标注
all_tags = {w.pos: w.pos_ for w in document}
&gt;&gt; {97:  u&apos;SYM&apos;, 98: u&apos;VERB&apos;, 99: u&apos;X&apos;, 101: u&apos;SPACE&apos;, 82: u&apos;ADJ&apos;, 83: u&apos;ADP&apos;, 84: u&apos;ADV&apos;, 87: u&apos;CCONJ&apos;, 88: u&apos;DET&apos;, 89: u&apos;INTJ&apos;, 90: u&apos;NOUN&apos;, 91: u&apos;NUM&apos;, 92: u&apos;PART&apos;, 93: u&apos;PRON&apos;, 94: u&apos;PROPN&apos;, 95: u&apos;PUNCT&apos;}

# document 中第一个句子的词性标注
for word in list(document.sents)[0]:  
    print word, word.tag_
&gt;&gt; ( Nice, u&apos;JJ&apos;) (place, u&apos;NN&apos;) (Better, u&apos;NNP&apos;) (than, u&apos;IN&apos;) (some, u&apos;DT&apos;) (reviews, u&apos;NNS&apos;) (give, u&apos;VBP&apos;) (it, u&apos;PRP&apos;) (creit, u&apos;NN&apos;) (for, u&apos;IN&apos;) (., u&apos;.&apos;)
</code></pre><p>来看一看 document 中的最常用词汇。我已经事先写好了预处理和文本数据清洗的函数。</p><pre><code>#一些参数定义
noisy_pos_tags = [“PROP”]
min_token_length = 2

#检查 token 是不是噪音的函数
def isNoise(token):     
    is_noise = False
    if token.pos_ in noisy_pos_tags:
        is_noise = True
    elif token.is_stop == True:
        is_noise = True
    elif len(token.string) &lt;= min_token_length:
        is_noise = True
    return is_noise
def cleanup(token, lower = True):
    if lower:
       token = token.lower()
    return token.strip()

# 评论中最常用的单词
from collections import Counter
cleaned_list = [cleanup(word.string) for word in document if not isNoise(word)]
Counter(cleaned_list) .most_common(5)
&gt;&gt; [( u&apos;hotel&apos;, 683), (u&apos;room&apos;, 652), (u&apos;great&apos;, 300),  (u&apos;sheraton&apos;, 285), (u&apos;location&apos;, 271)]
</code></pre><h3 id="2-3-实体识别"><a href="#2-3-实体识别" class="headerlink" title="2.3 实体识别"></a>2.3 实体识别</h3><p>spaCy 拥有一个快速实体识别模型，这个实体识别模型能够从 document 中找出实体短语。它能识别各种类型的实体，例如人名、位置、机构、日期、数字等。你可以通过“.ents”属性来读取这些实体。</p><p>下面让我们来获取我们 document 中所有类型的命名实体：</p><pre><code>labels = set([w.label_ for w in document.ents])
for label in labels:
    entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_]
    entities = list(set(entities))
    print label,entities
</code></pre><h3 id="2-4-依存句法分析"><a href="#2-4-依存句法分析" class="headerlink" title="2.4 依存句法分析"></a>2.4 依存句法分析</h3><p>spaCy 最强大的功能之一就是它可以通过调用轻量级的 API 来实现又快又准确的依存分析。这个分析器也可以用于句子边界检测以及区分短语块。依存关系可以通过“.children”、“.root”、“.ancestor”等属性读取。</p><pre><code># 取出所有句中包含“hotel”单词的评论
hotel = [sent for sent in document.sents if &apos;hotel&apos; in sent.string.lower()]

# 创建依存树
sentence = hotel[2] for word in sentence:
print word, &apos;: &apos;, str(list(word.children))
&gt;&gt; A :  []  cab :  [A, from]
from :  [airport, to]
the :  []
airport :  [the]
to :  [hotel]
the :  [] hotel :  
[the] can :  []
be :  [cab, can, cheaper, .]
cheaper :  [than] than :  
[shuttles]
the :  []
shuttles :  [the, depending]
depending :  [time] what :  []
time :  [what, of] of :  [day]
the :  [] day :  
[the, go] you :  
[]
go :  [you]
. :  []
</code></pre><p>解析所有居中包含“hotel”单词的句子的依存关系，并检查对于 hotel 人们用了哪些形容词。我创建了一个自定义函数，用于分析依存关系并进行相关的词性标注。</p><pre><code># 检查修饰某个单词的所有形容词
def pos_words (sentence, token, ptag):
    sentences = [sent for sent in sentence.sents if token in sent.string]     
    pwrds = []
    for sent in sentences:
        for word in sent:
            if character in word.string:
                   pwrds.extend([child.string.strip() for child in word.children
                                                      if child.pos_ == ptag] )
    return Counter(pwrds).most_common(10)

pos_words(document, &apos;hotel&apos;, “ADJ”)
&gt;&gt; [(u&apos;other&apos;, 20), (u&apos;great&apos;, 10), (u&apos;good&apos;, 7), (u&apos;better&apos;, 6), (u&apos;nice&apos;, 6), (u&apos;different&apos;, 5), (u&apos;many&apos;, 5), (u&apos;best&apos;, 4), (u&apos;my&apos;, 4), (u&apos;wonderful&apos;, 3)]
</code></pre><h3 id="2-5-名词短语（NP）"><a href="#2-5-名词短语（NP）" class="headerlink" title="2.5 名词短语（NP）"></a>2.5 名词短语（NP）</h3><p>依存树也可以用来生成名词短语：</p><pre><code># 生成名词短语
doc = nlp(u&apos;I love data science on analytics vidhya&apos;)
for np in doc.noun_chunks:
    print np.text, np.root.dep_, np.root.head.text
&gt;&gt; I nsubj love
   data science dobj love
   analytics pobj on
</code></pre><h2 id="3-集成词向量"><a href="#3-集成词向量" class="headerlink" title="3. 集成词向量"></a>3. 集成词向量</h2><p>spaCy 提供了内置整合的向量值算法，这些向量值可以反映词中的真正表达信息。它使用 <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a> 来生成向量。GloVe 是一种用于获取表示单词的向量的无监督学习算法。</p><p>让我们创建一些词向量，然后对其做一些有趣的操作吧：</p><pre><code>from numpy import dot
from numpy.linalg import norm
from spacy.en import English
parser = English()

# 生成“apple”的词向量 
apple = parser.vocab[u&apos;apple&apos;]

# 余弦相似性计算函数
cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))
others = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != unicode(&quot;apple&quot;)})

# 根据相似性值进行排序
others.sort(key=lambda w: cosine(w.vector, apple.vector))
others.reverse()


print &quot;top most similar words to apple:&quot;
for word in others[:10]:
    print word.orth_
&gt;&gt; apples iphone f ruit juice cherry lemon banana pie mac orange
</code></pre><h2 id="4-使用-spaCy-对文本进行机器学习"><a href="#4-使用-spaCy-对文本进行机器学习" class="headerlink" title="4. 使用 spaCy 对文本进行机器学习"></a>4. 使用 spaCy 对文本进行机器学习</h2><p>将 spaCy 集成进机器学习模型是非常简单、直接的。让我们使用 sklearn 做一个自定义的文本分类器。我们将使用 cleaner、tokenizer、vectorizer、classifier 组件来创建一个 sklearn 管道。其中的 tokenizer 和 vectorizer 会使用我们用 spaCy 自定义的模块构建。</p><pre><code>from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC

import string
punctuations = string.punctuation

from spacy.en import English
parser = English()

# 使用 spaCy 自定义 transformer
class predictors(TransformerMixin):
    def transform(self, X, **transform_params):
        return [clean_text(text) for text in X]
    def fit(self, X, y=None, **fit_params):
        return self
    def get_params(self, deep=True):
        return {}

# 进行文本清洗的实用的基本函数
def clean_text(text):     
    return text.strip().lower()
</code></pre><p>现在让我们使用 spaCy 的解析器和一些基本的数据清洗函数来创建一个自定义的 tokenizer 函数。值得一提的是，你可以用词向量来代替文本特征（使用深度学习模型效果会有较大的提升）</p><pre><code>#创建 spaCy tokenizer，解析句子并生成 token
#也可以用词向量函数来代替它
def spacy_tokenizer(sentence):
    tokens = parser(sentence)
    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != &quot;-PRON-&quot; else tok.lower_ for tok in tokens]
    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]     return tokens

#创建 vectorizer 对象，生成特征向量，以此可以自定义 spaCy 的 tokenizer
vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) classifier = LinearSVC()
</code></pre><p>现在可以创建管道，加载数据，然后运行分类模型了。</p><pre><code># 创建管道，进行文本清洗、tokenize、向量化、分类操作
pipe = Pipeline([(&quot;cleaner&quot;, predictors()),
                 (&apos;vectorizer&apos;, vectorizer),
                 (&apos;classifier&apos;, classifier)])

# Load sample data
train = [(&apos;I love this sandwich.&apos;, &apos;pos&apos;),          
         (&apos;this is an amazing place!&apos;, &apos;pos&apos;),
         (&apos;I feel very good about these beers.&apos;, &apos;pos&apos;),
         (&apos;this is my best work.&apos;, &apos;pos&apos;),
         (&quot;what an awesome view&quot;, &apos;pos&apos;),
         (&apos;I do not like this restaurant&apos;, &apos;neg&apos;),
         (&apos;I am tired of this stuff.&apos;, &apos;neg&apos;),
         (&quot;I can&apos;t deal with this&quot;, &apos;neg&apos;),
         (&apos;he is my sworn enemy!&apos;, &apos;neg&apos;),          
         (&apos;my boss is horrible.&apos;, &apos;neg&apos;)]
test =   [(&apos;the beer was good.&apos;, &apos;pos&apos;),     
         (&apos;I do not enjoy my job&apos;, &apos;neg&apos;),
         (&quot;I ain&apos;t feelin dandy today.&quot;, &apos;neg&apos;),
         (&quot;I feel amazing!&quot;, &apos;pos&apos;),
         (&apos;Gary is a good friend of mine.&apos;, &apos;pos&apos;),
         (&quot;I can&apos;t believe I&apos;m doing this.&quot;, &apos;neg&apos;)]

# 创建模型并计算准确率
pipe.fit([x[0] for x in train], [x[1] for x in train])
pred_data = pipe.predict([x[0] for x in test])
for (sample, pred) in zip(test, pred_data):
    print sample, pred
print &quot;Accuracy:&quot;, accuracy_score([x[1] for x in test], pred_data)

&gt;&gt;    (&apos;the beer was good.&apos;, &apos;pos&apos;) pos
      (&apos;I do not enjoy my job&apos;, &apos;neg&apos;) neg
      (&quot;I ain&apos;t feelin dandy today.&quot;, &apos;neg&apos;) neg
      (&apos;I feel amazing!&apos;, &apos;pos&apos;) pos
      (&apos;Gary is a good friend of mine.&apos;, &apos;pos&apos;) pos
      (&quot;I can&apos;t believe I&apos;m doing this.&quot;, &apos;neg&apos;) neg
      Accuracy: 1.0
</code></pre><h2 id="5-和其它库的对比"><a href="#5-和其它库的对比" class="headerlink" title="5. 和其它库的对比"></a>5. 和其它库的对比</h2><p>Spacy 是一个非常强大且具备工业级能力的 NLP 包，它能满足大多数 NLP 任务的需求。可能你会思考：为什么会这样呢？</p><p>让我们把 Spacy 和另外两个 python 中有名的实现 NLP 的工具 —— CoreNLP 和 NLTK 进行对比吧！</p><h3 id="支持功能表"><a href="#支持功能表" class="headerlink" title="支持功能表"></a>支持功能表</h3><table><thead><tr><th>功能</th><th>Spacy</th><th>NLTK</th><th>Core NLP</th></tr></thead><tbody><tr><td>简易的安装方式</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>Python API</td><td>Y</td><td>Y</td><td>N</td></tr><tr><td>多语种支持</td><td>N</td><td>Y</td><td>Y</td></tr><tr><td>分词</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>词性标注</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>分句</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>依存性分析</td><td>Y</td><td>N</td><td>Y</td></tr><tr><td>实体识别</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>词向量计算集成</td><td>Y</td><td>N</td><td>N</td></tr><tr><td>情感分析</td><td>Y</td><td>Y</td><td>Y</td></tr><tr><td>共指消解</td><td>N</td><td>N</td><td>Y</td></tr></tbody></table><h3 id="速度：主要功能（Tokenizer、Tagging、Parsing）速度"><a href="#速度：主要功能（Tokenizer、Tagging、Parsing）速度" class="headerlink" title="速度：主要功能（Tokenizer、Tagging、Parsing）速度"></a>速度：主要功能（Tokenizer、Tagging、Parsing）速度</h3><table><thead><tr><th><strong>库</strong></th><th><strong>Tokenizer</strong></th><th><strong>Tagging</strong></th><th><strong>Parsing</strong></th></tr></thead><tbody><tr><td>spaCy</td><td>0.2ms</td><td>1ms</td><td>19ms</td></tr><tr><td>CoreNLP</td><td>2ms</td><td>10ms</td><td>49ms</td></tr><tr><td>NLTK</td><td>4ms</td><td>443ms</td><td>–</td></tr></tbody></table><h3 id="准确性：实体抽取结果"><a href="#准确性：实体抽取结果" class="headerlink" title="准确性：实体抽取结果"></a>准确性：实体抽取结果</h3><table><thead><tr><th><strong>库</strong></th><th><strong>准确率</strong></th><th><strong>Recall</strong></th><th><strong>F-Score</strong></th></tr></thead><tbody><tr><td>spaCy</td><td>0.72</td><td>0.65</td><td>0.69</td></tr><tr><td>CoreNLP</td><td>0.79</td><td>0.73</td><td>0.76</td></tr><tr><td>NLTK</td><td>0.51</td><td>0.65</td><td>0.58</td></tr></tbody></table><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>本文讨论了 spaCy —— 这个基于 python，完全用于实现 NLP 的库。我们通过许多用例展示了 spaCy 的可用性、速度及准确性。最后我们还将其余其它几个著名的 NLP 库 —— CoreNLP 与 NLTK 进行了对比。</p><p>如果你能真正理解这篇文章要表达的内容，那你一定可以去实现各种有挑战的文本数据与 NLP 问题。</p><p>希望你能喜欢这篇文章，如果你有疑问、问题或者别的想法，请在评论中留言。</p><p>作者介绍：</p><p><a href="https://www.analyticsvidhya.com/blog/author/shivam5992/" target="_blank" rel="noopener">Shivam Bansal</a></p><p>Shivam Bansal 是一位数据科学家，在 NLP 与机器学习领域有着丰富的经验。他乐于学习，希望能解决一些富有挑战性的分析类问题。</p><ul><li><a href="https://twitter.com/shivamshaz" target="_blank" rel="noopener">https://twitter.com/shivamshaz</a></li><li><a href="https://www.linkedin.com/in/shivambansal1" target="_blank" rel="noopener">https://www.linkedin.com/in/shivambansal1</a></li><li><a href="https://github.com/shivam5992" target="_blank" rel="noopener">https://github.com/shivam5992</a></li></ul><blockquote><p>发布于掘金 <a href="https://juejin.im/post/5971a4b9f265da6c42353332" target="_blank" rel="noopener">https://juejin.im/post/5971a4b9f265da6c42353332</a></p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2017/07/10/你会给想学习机器学习的软件工程师提出什么建议？/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2017/07/10/你会给想学习机器学习的软件工程师提出什么建议？/" class="post-title-link" itemprop="https://lsvih.com/page/5/index.html">你会给想学习机器学习的软件工程师提出什么建议？</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2017-07-10 14:51:00" itemprop="dateCreated datePublished" datetime="2017-07-10T14:51:00+08:00">2017-07-10</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Translate/" itemprop="url" rel="index"><span itemprop="name">Translate</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>这很大一部分都取决于这名软件工程师的背景，以及他希望掌握机器学习的哪一部分。为了具体讨论，现在假设这是一名初级工程师，他读了 4 年本科，从业 2 年，现在想从事计算广告学（CA）、自然语言处理（NLP）、图像分析、社交网络分析、搜索、推荐排名相关领域。现在，让我们从机器学习的必要课程开始讨论（声明：下面的清单很不完整，如果您的论文没有被包括在内，提前向您抱歉）。</p><ul><li><p>线性代数<br>很多的机器学习算法、统计学原理、模型优化都依赖线性代数。这也解释了为何在深度学习领域 GPU 要优于 CPU。在线性代数方面，你至少得熟练掌握以下内容：</p><ul><li>标量、向量、矩阵、张量。你可以将它们看成零维、一维、二维、三维与更高维的对象，可以对它们进行各种组合、变换，就像乐高玩具一样。它们为数据变换提供了最基础的处理方法。</li><li>特征向量、标准化、矩阵近似、分解。实质上这些方法都是为了方便线性代数的运算。如果你想分析一个矩阵是如何运算的（例如检查神经网络中梯度消失问题，或者检查强化学习算法发散的问题），你得了解矩阵与向量应用了多少种缩放方法。而低阶矩阵近似与 Cholesky 分解可以帮你写出性能更好、稳定性更强的代码。</li><li>数值线性代数<br>如果你想进一步优化算法的话，这是必修课。它对于理解核方法与深度学习很有帮助，不过对于图模型及采样来说它并不重要。</li><li>推荐书籍<br><a href="http://www.amazon.com/Linear-Algebra-Undergraduate-Texts-Mathematics/dp/0387964126" target="_blank" rel="noopener">《Serge Lang, Linear Algebra》</a><br>很基础的线代书籍，很适合在校学生。<br><a href="http://www.amazon.com/Linear-Analysis-Introductory-Cambridge-Mathematical/dp/0521655773" target="_blank" rel="noopener">《Bela Bolobas, Linear Analysis》</a><br>这本书目标人群是那些想做数学分析、泛函分析的人。当然它的内容更加晦涩难懂，但更有意义。如果你攻读 PhD，值得一读。<br><a href="http://www.amazon.com/Numerical-Linear-Algebra-Lloyd-Trefethen/dp/0898713617" target="_blank" rel="noopener">《Lloyd Trefethen and David Bau, Numerical Linear Algebra》</a><br>这本书是同类书籍中较为推荐的一本。<a href="http://www.amazon.com/Numerical-Recipes-Scientific-Computing-Second/dp/0521431085/" target="_blank" rel="noopener">《Numerical Recipes》</a>也是一本不错的书，但是里面的算法略为过时了。另外，推荐 Golub 和 van Loan 合著的书<a href="http://www.amazon.com/Computations-Hopkins-Studies-Mathematical-Sciences/dp/1421407949/" target="_blank" rel="noopener">《Matrix Computations》</a>。</li></ul></li><li><p>优化与基础运算</p><p>大多数时候提出问题是很简单的，而解答问题则是很困难的。例如，你想对一组数据使用线性回归（即线性拟合），那么你应该希望数据点与拟合线的距离平方和最小；又或者，你想做一个良好的点击预测模型，那么你应该希望最大程度地提高用户点击广告概率估计的准确性。也就是说，在一般情况下，我们会得到一个客观问题、一些参数、一堆数据，我们要做的就是找到通过它们解决问题的方法。找到这种方法是很重要的，因为我们一般得不到闭式解。</p><ul><li><p>凸优化</p><p>在大多情况下，优化问题不会存在太多的局部最优解，因此这类问题会比较好解决。这种“局部最优即全局最优”的问题就是凸优化问题。</p><p>（如果你在集合的任意两点间画一条直线，整条线始终在集合范围内，则这个集合是一个凸集合；如果你在一条函数曲线的任意两点间画一条直线，这两点间的函数曲线始终在这条直线之下，则这个函数是一个凸函数）</p><p>Steven Boyd 与 Lieven Vandenberghe <a href="http://stanford.edu/~boyd/cvxbook/" target="_blank" rel="noopener">合著的书</a>可以说是这个领域的规范书籍了，这本书非常棒，而且是免费的，值得一读；此外，你可以在 <a href="http://web.stanford.edu/~boyd/" target="_blank" rel="noopener">Boyd 的课程</a>中找到很多很棒的幻灯片；<a href="http://www.mit.edu/~dimitrib/home.html" target="_blank" rel="noopener">Dimitri Bertsekas</a> 写了一系列关于优化、控制方面的书籍。读通这些书足以让任何一个人在这个领域立足。</p></li><li><p>随机梯度下降（SGD）</p><p>大多数问题其实最开始都是凸优化问题的特殊情况（至少早期定理如此），但是随着数据的增加，凸优化问题的占比会逐渐减少。因此，假设你现在得到了一些数据，你的算法将会需要在每一个更新步骤前将所有的数据都检查一遍。</p><p>现在，我不怀好意地给了你 10 份相同的数据，你将不得不重复 10 次没有任何帮助的工作。不过在现实中并不会这么糟糕，你可以设置很小的更新迭代步长，每次更新前都将所有的数据检查一遍，这种方法将会帮你解决这类问题。小步长计算在机器学习中已经有了很大的转型，配合上一些相关的算法会使得解决问题更加地简单。</p><p>不过，这样的做法对并行化计算提出了挑战。我们于 2009 年发表的<a href="http://arxiv.org/abs/0911.0491" target="_blank" rel="noopener">《Slow Learners are Fast》</a>论文可能就是这个方向的先导者之一。2013 年牛峰等人发表的<a href="https://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf" target="_blank" rel="noopener">《Hogwild》</a>论文给出了一种相当优雅的无锁版本变体。简而言之，这类各种各样的算法都是通过在单机计算局部梯度，并异步更新共有的参数集实现并行快速迭代运算。</p><p>随机梯度下降的另一个难题就是如何控制过拟合（例如可以通过正则化加以控制）。另外还有一种解决凸优化的惩罚方式叫近端梯度算法（PGD）。最流行的当属 Amir Beck 和 Marc Teboulle 提出的 <a href="http://people.rennes.inria.fr/Cedric.Herzet/Cedric.Herzet/Sparse_Seminar/Entrees/2012/11/12_A_Fast_Iterative_Shrinkage-Thresholding_Algorithmfor_Linear_Inverse_Problems_(A._Beck,_M._Teboulle" target="_blank" rel="noopener">FISTA 算法</a>_files/Breck_2009.pdf)了。相关代码可以参考 Francis Bach 的 <a href="http://spams-devel.gforge.inria.fr/" target="_blank" rel="noopener">SPAM toolbox</a>。</p></li><li><p>非凸优化方法</p><p>许多的机器学习问题是非凸的。尤其是与深度学习相关的问题几乎都是非凸的，聚类、主题模型（topic model）、潜变量方法（latent variable method）等各种有趣的机器学习方法也是如此。一些最新的加速技术将对此有所帮助。例如我的学生 <a href="http://www.cs.cmu.edu/~sjakkamr/" target="_blank" rel="noopener">Sashank Reddy</a> 最近展示了如何在这种情况下得到良好的<a href="http://arxiv.org/abs/1603.06160" target="_blank" rel="noopener">收敛</a><a href="http://arxiv.org/abs/1603.06159" target="_blank" rel="noopener">速率</a>。</p><p>也可以用一种叫做谱学习算法（Spectral Method）的技术。<a href="http://newport.eecs.uci.edu/anandkumar/" target="_blank" rel="noopener">Anima Anandkumar</a> 在最近的 <a href="/profile/Anima-Anandkumar-1">Quora session</a> 中详细地描述了这项技术的细节。请仔细阅读她的文章，因为里面干货满满。简而言之，凸优化问题并不是唯一能够可靠解决的问题。在某些情况中你可以试着找出其问题的数学等价形式，通过这样找到能够真正反映数据中聚类、主题、相关维度、神经元等一切信息的参数。如果你愿意且能够将一切托付给数学解决，那是一件无比伟大的事。</p><p>最近，在深度神经网络训练方面涌现出了各种各样的新技巧。我将会在下面介绍它们，但是在一些情况中，我们的目标不仅仅是优化模型，而是找到一种特定的解决方案（就好像旅途的重点其实是过程一样）。</p></li></ul></li><li><p>（分布式）系统</p><p>机器学习之所以现在成为了人类、测量学、传感器及数据相关领域几乎是最常用的工具，和过去 10 年规模化算法的发展密不可分。<a href="http://research.google.com/pubs/jeff.html" target="_blank" rel="noopener">Jeff Dean</a> 过去的一年发了 6 篇机器学习教程并不是巧合。在此简单介绍一下他：<a href="http://www.informatika.bg/jeffdean" target="_blank" rel="noopener">点击查看</a>，他是 MapReduce、GFS 及 BigTable 等技术背后的创造者，正是这些技术让 Google 成为了伟大的公司。</p><p>言归正传，（分布式）系统研究为我们提供了分布式、异步、容错、规模化、简单（Simplicity）的宝贵工具。最后一条“简单”是机器学习研究者们常常忽视的一件事。简单（Simplicity）不是 bug，而是一种特征。下面这些技术会让你受益良多：</p><ul><li><p>分布式哈希表</p><p>它是 <a href="https://memcached.org/" target="_blank" rel="noopener">memcached</a>、<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" rel="noopener">dynamo</a>、<a href="http://research.microsoft.com/en-us/um/people/antr/PAST/pastry.pdf" target="_blank" rel="noopener">pastry</a> 以及 <a href="http://docs.ceph.com/docs/hammer/rados/" target="_blank" rel="noopener">ceph</a> 等的技术基础。它们所解决的都是同一件事情 —— 如何将对象分发到多台机器上，从而避免向中央存储区提出请求。为了达到这个目的，你必须将数据位置进行随机但确定的编码（即哈希）。另外，你需要考虑到当有机器出现故障时的处理方式。</p><p>我们自己的参数服务器就是使用这种<a href="https://www.cs.cmu.edu/~dga/papers/osdi14-paper-li_mu.pdf" target="_blank" rel="noopener">数据布局</a>。这个项目的幕后大脑是我的学生 <a href="http://www.cs.cmu.edu/~muli/" target="_blank" rel="noopener">Mu Li</a> 。请参阅 <a href="http://dmlc.ml/" target="_blank" rel="noopener">DMLC</a> 查看相关的工具集。</p></li><li><p>一致性与通信</p><p>这一切的基础都是 Leslie Lamport 的 <a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf" target="_blank" rel="noopener">PAXOS</a> 协议。它解决了不同机器（甚至部分机器不可用）的一致性问题。如果你曾经使用过版本控制工具，你应该可以直观地明白它是如何运行的——比如你有很多机器（或者很多开发者）都在进行数据更新（或更新代码），在它们（他们）不随时进行交流的情况下，你会如何将它们（他们）结合起来（不靠反复地求 diff）?</p><p>在（分布式）系统中，解决方案是一个叫做向量时钟的东西（请参考 Google 的 <a href="http://blogoscoped.com/archive/2008-07-24-n69.html" target="_blank" rel="noopener">Chubby</a>）。我们也在参数服务器上使用了这种向量时钟的变体，这个变体与本体的区别就是我们仅使用向量时钟来限制参数的范围（Mu Li 做的），这样可以确保内存不会被无限增长的向量时钟时间戳给撑爆，正如文件系统不需要给每个字节都打上时间戳。</p></li><li><p>容错机制、规模化与云</p><p>学习这些内容最简单的方法就是在云服务器上运行各种算法，至于云服务可以找 <a href="http://aws.amazon.com" target="_blank" rel="noopener">Amazon AWS</a>、<a href="http://console.google.com" target="_blank" rel="noopener">Google GWC</a>、<a href="http://azure.microsoft.com" target="_blank" rel="noopener">Microsoft Azure</a> 或者 <a href="http://serverbear.com/" target="_blank" rel="noopener">其它各种各样的服务商</a>。一次性启动 1,000 台服务器，意识到自己坐拥如此之大的合法“僵尸网络”是多么的让人兴奋！之前我在 Google 工作，曾在欧洲某处接手 5,000 余台高端主机作为主题模型计算终端，它们是我们通过能源法案获益的核电厂相当可观的一部分资源。我的经理把我带到一旁，偷偷告诉我这个实验是多么的昂贵……</p><p>可能入门这块最简单的方法就是去了解 <a href="http://www.docker.com" target="_blank" rel="noopener">docker</a> 了吧。现在 docker 团队已经开发了大量的规模化工具。特别是他们最近加上的 <a href="https://docs.docker.com/machine/" target="_blank" rel="noopener">Docker Machine</a> 和 <a href="https://docs.docker.com/cloud/" target="_blank" rel="noopener">Docker Cloud</a>，可以让你就像使用打印机驱动一样连接云服务。</p></li><li><p>硬件</p><p>说道硬件可能会让人迷惑，但是如果你了解你的算法会在什么硬件上运行，对优化算法是很有帮助的。这可以让你知道你的算法是否能在任何条件下保持巅峰性能。我认为每个入门者都应该看看 Jeff Dean 的 <a href="https://gist.github.com/jboner/2841832" target="_blank" rel="noopener">《每个工程师都需要记住的数值》</a>。我在面试时最喜欢的问题（至少现在最喜欢）就是“请问你的笔记本电脑有多快”。了解是什么限制了算法的性能是很有用的：是缓存？是内存带宽？延迟？还是磁盘？或者别的什么？<a href="http://www.anandtech.com" target="_blank" rel="noopener">Anandtech</a> 在微处理器架构与相关方面写了很多很好的文章与评论，在 Intel、ARM、AMD 发布新硬件的时候不妨去看一看他的评论。</p></li></ul></li><li><p>统计学</p><p>我故意把这块内容放在文章的末尾，因为几乎所有人都认为它是（它的确是）机器学习的关键因而忽视了其它内容。统计学可以帮你问出好的问题，也能帮你理解你的建模与实际数据有多接近。</p><p>大多数图模型、核方法、深度学习等都能从“问一个好的问题”得到改进，或者说能够定义一个合理的可优化的目标函数。</p><ul><li><p>统计学相关资料<br><a href="http://www.stat.cmu.edu/~larry/" target="_blank" rel="noopener">Larry Wasserman</a> 的书<a href="http://www.stat.cmu.edu/~larry/all-of-statistics/" target="_blank" rel="noopener">《All of Statistics》</a>很好地介绍了统计学。或者你也可以看看 David McKay 的 <a href="http://www.inference.phy.cam.ac.uk/itprnn/book.pdf" target="_blank" rel="noopener">《Machine Learning》</a>一书，它是免费的，内容丰富而全面。此外还有很多好书值得一看，例如 <a href="https://mitpress.mit.edu/books/machine-learning-0" target="_blank" rel="noopener">Kevin Murphy</a> 的、<a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" target="_blank" rel="noopener">Chris Bishop</a> 的、以及 <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" target="_blank" rel="noopener">Trevor Hastie、Rob Tibshirani 与 Jerome Friedman</a> 合著的书。还有，Bernhard Scholkopf 和我也<a href="https://mitpress.mit.edu/books/learning-kernels" target="_blank" rel="noopener">写了一本</a>。</p></li><li><p>随机算法与概率计算</p><p>统计学算法本质上也是个计算机科学方面的问题。但是统计学的算法与计算机科学的最大区别在于，统计学是将计算机作为一个工具来设计算法，而不是作为一个黑箱进行调参。我很喜欢<a href="http://www.amazon.com/Probability-Computing-Randomized-Algorithms-Probabilistic/dp/0521835402" target="_blank" rel="noopener">这本 Michael Mitzenmacher 与 Eli Upfal 合著的书</a>，它涵盖了很多方面的问题，并且很容易读懂。另外如果你想更深入地了解这个“工具”，请阅读<a href="http://www.amazon.com/Randomized-Algorithms-Rajeev-Motwani/dp/0521474655" target="_blank" rel="noopener">这本 Rajeev Motwani 和 Prabhakar Raghavan 合著的书籍</a>。这本书写的很棒，但是没有统计学背景很难理解它。</p></li></ul></li></ul><p>这篇文章已经写的够久了，不知道有没有人能读到这里，我要去休息啦。现在网上有很多很棒的视频内容可以帮助你学习，许多教师现在都开通了他们的 Youtube 频道，上传他们的上课内容。这些课程有时可以帮你解决一些复杂的问题。这儿是<a href="https://www.youtube.com/user/smolix/playlists" target="_blank" rel="noopener">我的 Youtube 频道</a>欢迎订阅。顺便推荐 <a href="https://www.youtube.com/user/ProfNandoDF" target="_blank" rel="noopener">Nando de Freitas 的 Youtube 频道</a>，他比我讲得好多了。</p><p>最后推荐一个非常好用的工具：<a href="http://www.dmlc.ml" target="_blank" rel="noopener">DMLC</a>。它很适合入门，包含了大量的分布式、规模化的机器学习算法，还包括了通过 MXNET 实现的神经网络。</p><p>虽然本文还有很多方面没有提到（例如编程语言、数据来源等），但是这篇文章已经太长了，这些内容请参考其他文章吧~</p><blockquote><p>发布于掘金 <a href="https://juejin.im/post/596323416fb9a06bae1dff63" target="_blank" rel="noopener">https://juejin.im/post/596323416fb9a06bae1dff63</a></p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">lsvih</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">138</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">165</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/lsvih" title="GitHub &rarr; https://github.com/lsvih" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:lsvih@qq.com" title="E-Mail &rarr; mailto:lsvih@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">lsvih</span></div><div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.5.0</div><div class="footer-custom"><a target="_blank" rel="external nofollow" href="http://www.miitbeian.gov.cn">京ICP备18029472号</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script><link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css"><script src="/lib/algolia-instant-search/instantsearch.min.js"></script><script src="/js/src/algolia-search.js?v=6.5.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>