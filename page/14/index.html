<!DOCTYPE html><html class="theme-next pisces"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0"><link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"6.5.0",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!1,fastclick:!1,lazyload:!1,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},algolia:{applicationID:"6U6P1RGK4F",apiKey:"b14e73cdd627eabe947b5decbe14850f",indexName:"lsvih",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><meta property="og:type" content="website"><meta property="og:title" content="My note"><meta property="og:url" content="https://lsvih.com/page/14/index.html"><meta property="og:site_name" content="My note"><meta property="og:locale" content="default"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="My note"><link rel="canonical" href="https://lsvih.com/page/14/"><script type="text/javascript" id="page.configurations">CONFIG.page={sidebar:""}</script><title>My note – lsvih</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?4de170e51b04cee02fbdb1b7cb7a8a60";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style type="text/css">.sidebar-inner,.use-motion .brand,.use-motion .collection-title,.use-motion .comments,.use-motion .menu-item,.use-motion .motion-element,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .logo,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage" lang="default"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">My note</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">lsvih</h1></div><div class="site-nav-toggle"><button aria-label="Toggle navigation bar"><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="algolia-popup popup search-popup"><div class="algolia-search"><div class="algolia-search-input-icon"><i class="fa fa-search"></i></div><div class="algolia-search-input" id="algolia-search-input"></div></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/24/机器学习学习笔记（六）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/24/机器学习学习笔记（六）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（六）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-24 08:31:00" itemprop="dateCreated datePublished" datetime="2016-03-24T08:31:00+08:00">2016-03-24</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>在一些线性回归问题中，可以用正规方程方法更快地解决问题，直接解出参数值来。</p><p>设特征矩阵为X（第一列是$x_0=1$）,训练集结果为向量y，能够带入公式</p><p>$$\theta = (X^T X)^{-1}X^T y$$</p><p>直接解出$\theta$的向量。</p><p>举例：</p><p><img src="/images/pasted-23.png" alt="upload successful"></p><p>带入公式进行矩阵运算即可得出结果。<br>以此题为例，我使用 Octave 进行了运算</p><p>先输入输入值X的矩阵</p><p><img src="/images/pasted-24.png" alt="upload successful"></p><p>输入输出值y的向量</p><p><img src="/images/pasted-25.png" alt="upload successful"></p><p>直接带入公式</p><p>$$\theta = (X^T X)^{-1}X^T y$$</p><p><img src="/images/pasted-26.png" alt="upload successful"></p><hr><h2 id="梯度下降法与正规方程的对比"><a href="#梯度下降法与正规方程的对比" class="headerlink" title="梯度下降法与正规方程的对比"></a>梯度下降法与正规方程的对比</h2><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率$\alpha$</td><td>不需要</td></tr><tr><td>需要多次迭代</td><td>一次运算得出</td></tr><tr><td>当特征数量 n 大时也能较好适用</td><td>需要计算$(X^TX)^{-1}$ 如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为 $O(n^3)$，通常来说当 n 小于 10000 时还可以接受</td></tr><tr><td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其它模型</td></tr></tbody></table></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/24/机器学习学习笔记（五）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/24/机器学习学习笔记（五）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（五）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-24 07:01:00" itemprop="dateCreated datePublished" datetime="2016-03-24T07:01:00+08:00">2016-03-24</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="特征缩放（Feature-Scaling）"><a href="#特征缩放（Feature-Scaling）" class="headerlink" title="特征缩放（Feature Scaling）"></a>特征缩放（Feature Scaling）</h2><p>在线性回归的实际例子中中多数会遇到这样的情况，某两个特征值的数值相差很远，例如房价可能是100(万元)的数量级，房屋面积可能是100的数量级。表现在轮廓图上的形式就是如下图一样，</p><p><img src="/images/pasted-7.png" alt="upload successful"></p><p>等高线（代价函数J）的形状呈现出非常细长的椭圆。</p><p><img src="/images/pasted-19.png" alt="upload successful"></p><p>可以看到在这种情况下的梯度下降函数需要迭代很多次，在最小值附近震荡直到最后收敛。为了使梯度下降能够尽量朝着一个方向有效率地进行，我们需要对输入值中的某些特征值进行统一的缩放。</p><p><img src="/images/pasted-20.png" alt="upload successful"></p><p>这样的参数值接近的代价方程，梯度下降函数的迭代就会方便与均匀很多，从而达到减少迭代次数的目的。一般来说，将两个参数都缩放到-1，1之间较为合适。特征缩放可直接参照公式：</p><p>$$x_n=\frac{x_n-\mu _n}{S_n}$$</p><p>其中 $\mu_n$ 是平均值，$S_n$ 是标准差。</p><blockquote><p>为了简单方便，可以直接用最大值-最小值来代替标准差。</p></blockquote><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>学习率α在梯度下降方程中起着重要的作用，梯度下降算法的每次迭代受到学习率的影响，如果学习率 α过小，则达到收敛所需的迭代次数会非常高；如果学习率α 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p><p>在一个线性回归情况中尝试多个学习率可以找到计算效率最高的学习率。通常可以考虑尝试些学习率：α=0.01，0.03，0.1，0.3，1，3，10 （它们都是3倍3倍地增加）</p><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><p>在现实情况中，往往线性回归是不足以很好的契合一个数据集的。</p><p>例如</p><p><img src="/images/pasted-21.png" alt="upload successful"></p><p>用的三次方程，</p><p>其实也可以化为线性方程看：</p><p><img src="/images/pasted-22.png" alt="upload successful"></p><p>但是经过平方立方之后，每个参数的差值会变得更大，因此特征缩放在这样的情况中更加的重要。</p><p>比如上面如果size为1到1000的话，x2就是1到1000000，x3就是1到1000000000。不作处理会使算法很难正常进行下去。可以使x1=size/1000，x2=size^2/1000000,x3=size^3/1000000000这样就能使几个参数的值在近似的范围内。</p><p>再例如</p><p>$$h_\theta(x)=\theta_0+\theta_1(size)+\theta_2\sqrt{(size)}$$</p><p>要看成</p><p>$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$$</p><p>线性方程，size在1-1000间，那么$x_1=\frac{size}{1000},x_2=\frac{\sqrt{size}}{\sqrt{1000}}$</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/24/css控制文字自动换行并隐藏超出的部分/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/24/css控制文字自动换行并隐藏超出的部分/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">css控制文字自动换行并隐藏超出的部分</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-24 02:05:00" itemprop="dateCreated datePublished" datetime="2016-03-24T02:05:00+08:00">2016-03-24</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Front-End/" itemprop="url" rel="index"><span itemprop="name">Front End</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">display</span>: <span class="selector-tag">-webkit-box</span>;</span><br><span class="line"><span class="selector-tag">-webkit-line-clamp</span>: 3;</span><br><span class="line"><span class="selector-tag">-webkit-box-orient</span>: <span class="selector-tag">vertical</span>;</span><br><span class="line"><span class="selector-tag">overflow</span>: <span class="selector-tag">hidden</span>;</span><br></pre></td></tr></table></figure><p><code>-webkit-line-clamp</code>的数字代表文字最大为多少行。超出部分会用”…”代替</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/23/机器学习学习笔记（四）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/23/机器学习学习笔记（四）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（四）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-23 18:02:00" itemprop="dateCreated datePublished" datetime="2016-03-23T18:02:00+08:00">2016-03-23</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="多元线性回归与梯度下降的推广"><a href="#多元线性回归与梯度下降的推广" class="headerlink" title="多元线性回归与梯度下降的推广"></a>多元线性回归与梯度下降的推广</h3><p>已经理解并明白了单变量线性回归与其对应的两个参数的梯度下降方程，现将情况推广到多元的情况。</p><p>例如最初的例子房价与房屋大小的关系，现在可以加上卧室数量、大厅数量、房屋年龄等其他的参数一并参与考虑来进行预测。</p><table><thead><tr><th>Size($feet^2$)</th><th>Number of bedrooms</th><th>Number of floors</th><th>Age of home(years)</th><th>Price($1000)</th></tr></thead><tbody><tr><td>2104</td><td>5</td><td>1</td><td>45</td><td>460</td></tr><tr><td>1416</td><td>3</td><td>2</td><td>40</td><td>232</td></tr><tr><td>1534</td><td>3</td><td>2</td><td>30</td><td>315</td></tr><tr><td>852</td><td>2</td><td>1</td><td>36</td><td>178</td></tr><tr><td>…</td><td>…</td><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>在单变量线性回归中，样本的数量被记作m，在多元的情况下依然记为m，例如上表的数据有多少行m就为多少。</p><p>同时上表较之前多了几列，其特征数量即为n，即除了price之外的列数即为n,n=4。</p><p>同样将数据分为输入数据x与输出数据y，以m上标n下标的形式表现。例如上表的第三行表示的房子的层数就用$x^{(3)}_3$来表示。</p><p>因此由原来简单的</p><p>$$h_\theta(x)=\theta_0+\theta_1x$$</p><p>推广开来，成为了如下形式</p><p>$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$</p><p>为了定义方便，设x0=1,能得到对多元与单变量都适用的预测方程：</p><p>$$h(x)=\sum^n_{i=0}\theta_ix_i=\theta^TX$$</p><hr><p>同样的，多元线性回归的预测方程也需要有值来评估预测拟合度的好坏，因此引入之前的代价方程，推广到多元情况，很容易得到代价方程(Cost Function)：</p><p>$$J(\theta_0,\theta_1,…,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$</p><p>对于单变量线性回归来说，梯度下降的原理如下：</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p>而对于多元线性回归来说，$J(\theta_0,\theta_1)$推广到了$J(\theta_0,…,\theta_n)$，因此求偏导得到的结果有n种情况。</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><p>偏导得</p><p><img src="/images/pasted-18.png" alt="upload successful"></p><p>$$ \theta_0:=\theta_0 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_0$$<br>$$ \theta_1:=\theta_1 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_1$$<br>$$ \theta_2:=\theta_2 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_2$$</p><p>可以发现，最终得到的公式仍然符合单变量线性回归（n=1）。</p><blockquote><p>偏导过程可由高数解出。</p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/23/机器学习学习笔记（三）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/23/机器学习学习笔记（三）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（三）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-23 16:48:56" itemprop="dateCreated datePublished" datetime="2016-03-23T16:48:56+08:00">2016-03-23</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>把一个参数的情形推广到两个参数，每个$\theta_0$都对应一组J与$\theta_1$,表现在坐标图上呈现出一个碗状图形</p><p><img src="/images/pasted-6.png" alt="upload successful"></p><hr><p>用了等高线的概念来理解这个图，可以做出它的“等高图”（轮廓图，contour figure）</p><p><img src="/images/pasted-7.png" alt="upload successful"></p><p>横坐标与纵坐标便代表了两个参数，不同的圆弧代表了不同的代价函数值。</p><p>发现通过不断地降低J的值，可以得到更好的预测函数的拟合。</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><p><img src="/images/pasted-9.png" alt="upload successful"></p><p><img src="/images/pasted-10.png" alt="upload successful"></p><hr><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>为了让J尽可能的小，因此要同时考虑$\theta_1$与$\theta_0$的值。如果$\theta_1$保持不动，$\theta_0$就要往得到更小J的方向取值，反之亦然。可以理解成人站在山坡上，一步一步地向地势低的地方走，最终会到达“局部最低点”（local minimum）。</p><p><img src="/images/pasted-11.png" alt="upload successful"></p><p>于是可以看到，影响这个过程的有几个因素：起始的位置，每步的步长和每个点的某方向上的斜率。</p><p><img src="/images/pasted-12.png" alt="upload successful"></p><p>发现不同的起始位置可能会到达不同的局部最小值。因为这种“下山”的方式不是全面的，没有测算两个参数的所有情况。</p><hr><p>回到只有一个参数的情况上看。</p><p>$\theta_1:=\theta_1-\alpha\frac{d}{d\theta_1}J(\theta_1)$（ := 符号代表赋值）</p><p>$\theta_1$连续不断地减去当时的斜率与步长（合适的步长）的乘积，会向局部最小值滑动（可用高数证明），越接近局部最小值斜率就越小，值移动的就越慢，最后无限趋近到达导数为0的点。</p><p><img src="/images/pasted-13.png" alt="upload successful"></p><p>当步长的取值过大时，$\theta_1$有可能直接滑过局部最小值而到了极值点的另一侧，此时导数符号反转，与上面的趋近最小值的过程刚好相反，$\theta_1$越来越远离这个点。</p><p><img src="/images/pasted-14.png" alt="upload successful"></p><blockquote><p>但是当步长取值过小时，找到局部最小值的过程会变得太过漫长，效率低下<br>by the way,如果起始点一开始就在一个极小值点上时，其导数为0，它不再会发生变化</p></blockquote><p>为了得到收敛值，可列出如下算式。</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p>带入原式得</p><p><img src="/images/pasted-16.png" alt="upload successful"></p><p>这就是梯度下降算法。</p><hr><p>梯度下降算法能得到一个局部最优解，而对于线性回归问题来说，它的坐标图形是碗状的，只有一个最低点，因此用梯度下降法得到的点就是线性回归问题的全局最低值，使J最小，即全局最优解。</p><blockquote><p>梯度下降算法一定是两个参数同时下降才能达到效果</p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/23/机器学习学习笔记（二）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/23/机器学习学习笔记（二）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（二）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-23 16:21:12" itemprop="dateCreated datePublished" datetime="2016-03-23T16:21:12+08:00">2016-03-23</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>在监督学习中，有一种情况是数据集呈连续状分布，数据的几个值大致呈线性关系（如例子中的房价和房间大小的关系）。体现在坐标轴上如图下所示</p><p><img src="/images/pasted-3.png" alt="upload successful"></p><p>可以画出一条直线大概描述出整体数据的走向，这条直线被称为拟合曲线，在机器学习中直线代表的方程被称为预测（Hypothesis）方程，可以记作</p><p>$$ h_\theta(x)=\theta_0+\theta_1x $$</p><p>的形式，其中两个θ被称为预测方程的参数（Parameters），直接影响预测方程预测的准确性。</p><hr><p>为了方便记录，整个数据集包含的样品数（坐标图上的所有点数）记作m，</p><p>每个点的x与y轴坐标一一对应，带入x值可以通过预测方程预测出y的大概值。因此称x为输入值(input)，y为输出值(output)</p><hr><p>为了表示预测方程预测的准确性，可以拿每个输入值x带入预测方程计算得到预测值h(x)，再与实际值y进行比较。通过这种比较，能够衡量在这组参数下预估的结果和实际结果的差距，这样的计算方式称为代价函数（Cost Fuction）。比如说线性回归的代价函数定义为:</p><p>$$ J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 $$</p><p>（误差平方和函数是一个比较合适、常用的选择，当然，也可以选择一些其他形式的代价函数）</p><p>代价函数值越小，就说明当前参数值下的预测方程越准确。</p><p>为了得到尽量准确的预测方程，就要使J(代价函数)的值最小。现在单独将参数一个个拿出来看：</p><p>假设$\theta_0=0$，即预测函数的直线经过原点。假定训练集为(1,1)(2,2)(3,3)，很明显斜率为1的时候代价函数最小。</p><p>此时J=0,说明联系集的所有值全部符合预测方程。</p><blockquote><p>J=0时能说明预测方程符合程度较好，但不能说明接下来的数一定能够被完美预测。</p></blockquote><p>当让$\theta_1=0.5$时，可以发现坐标图变成了如下所示</p><p><img src="/images/pasted-4.png" alt="upload successful"></p><p>预测函数明显偏离了数据集。这时的代价函数值为</p><p>$$J(0)=\frac{1}{2 \times 3}[(0.5-1)^2+(1-2)^2+(1.5-3)^2]=\frac{3.5}{6}=0.58$$</p><p>当$\theta_1=0$时，</p><p>$$J(0)=\frac{1}{2 \times 3}[1^2+2^2+3^2]=\frac{14}{6}=2.3$$</p><p>可以发现偏差的越大，代价函数值越大。而且由于代价函数是平方差，因此参数往另一个方向变化也会有对称的变化。模型所预测的值与训练集中实际值之间的差距就是建模误差（modeling error）。</p><p>可以将代价函数值与参数$\theta_1$的值对应起来，坐标图如下</p><p><img src="/images/pasted-5.png" alt="upload successful"></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/23/vuejs的嵌套列表绑定/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/23/vuejs的嵌套列表绑定/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">vuejs 的嵌套列表绑定</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-23 11:49:00" itemprop="dateCreated datePublished" datetime="2016-03-23T11:49:00+08:00">2016-03-23</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Front-End/" itemprop="url" rel="index"><span itemprop="name">Front End</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>在项目中碰到了在v-for列表内还有一层列表的情况</p><p>json结构如下：<br></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"orders"</span>:</span><br><span class="line">[&#123;<span class="string">"id"</span>:<span class="string">"1"</span>,</span><br><span class="line"><span class="string">"state"</span>:<span class="string">"0"</span>,</span><br><span class="line"><span class="string">"time"</span>:<span class="string">"2016-03-17"</span>,</span><br><span class="line"><span class="string">"goods"</span>:[&#123;<span class="string">"id"</span>:<span class="string">"1"</span>,<span class="string">"name"</span>:<span class="string">"物品1"</span>,<span class="string">"img"</span>:<span class="string">"xxx.jpg"</span>&#125;,</span><br><span class="line">&#123;<span class="string">"id"</span>:<span class="string">"1"</span>,<span class="string">"name"</span>:<span class="string">"物品1"</span>,<span class="string">"img"</span>:<span class="string">"xxx.jpg"</span>&#125;]&#125;,</span><br><span class="line">&#123;<span class="string">"id"</span>:<span class="string">"2"</span>,</span><br><span class="line"><span class="string">"state"</span>:<span class="string">"1"</span>,</span><br><span class="line"><span class="string">"time"</span>:<span class="string">"2016-03-17"</span>,</span><br><span class="line"><span class="string">"goods"</span>:[&#123;<span class="string">"id"</span>:<span class="string">"1"</span>,<span class="string">"name"</span>:<span class="string">"物品1"</span>,<span class="string">"img"</span>:<span class="string">"xxx.jpg"</span>&#125;,</span><br><span class="line">&#123;<span class="string">"id"</span>:<span class="string">"1"</span>,<span class="string">"name"</span>:<span class="string">"物品1"</span>,<span class="string">"img"</span>:<span class="string">"xxx.jpg"</span>&#125;]&#125;</span><br><span class="line">]&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到需要在列表内嵌另一个列表。绑定代码为<br></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> Vue(&#123;</span><br><span class="line">	el: <span class="string">"#allorder"</span>,</span><br><span class="line">	data: order</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p></p><p>html代码为<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">id</span>=<span class="string">"allorder"</span> <span class="attr">v-for</span>=<span class="string">"orders in orders"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"order-state"</span>&gt;</span>&#123;&#123;orders.state&#125;&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">   订单号:<span class="tag">&lt;<span class="name">span</span>&gt;</span>&#123;&#123;orders.id&#125;&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">   下单日期:<span class="tag">&lt;<span class="name">span</span>&gt;</span>&#123;&#123;orders.ordertime&#125;&#125;<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"lineimg"</span> <span class="attr">v-for</span>=<span class="string">"good in orders.goods"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"item"</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">v-bind:src</span>=<span class="string">"good.img_src"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br></pre></td></tr></table></figure><p></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://lsvih.com/2016/03/23/机器学习学习笔记（一）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="lsvih"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="My note"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a href="/2016/03/23/机器学习学习笔记（一）/" class="post-title-link" itemprop="https://lsvih.com/page/14/index.html">机器学习学习笔记（一）</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2016-03-23 01:08:04" itemprop="dateCreated datePublished" datetime="2016-03-23T01:08:04+08:00">2016-03-23</time> </span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>the field of study that gives computers the ability to learn without being explicitly programmed</p></blockquote><p>机器学习让计算机有能力去自主学习，而不是被死板地编程，它发源于人工智能领域，在现在各个领域发挥了独特的作用。</p><h4 id="应用示例"><a href="#应用示例" class="headerlink" title="应用示例"></a>应用示例</h4><p>谷歌百度等搜索引擎实现的学习算法学会如何对网页排名，从而让海量的网页有序地展示在用户面前；</p><p>Iphone等手机与某些app在处理照片时，能够自动地识别出照片中的人；</p><p>电子邮箱系统在用户对广告邮件等标记垃圾邮件后，垃圾邮件过滤器学会了如何自动甄选出垃圾邮件，让用户避免垃圾邮件的困扰；</p><p>无人驾驶汽车通过图像或雷达等传感器学习大量正常驾驶时车辆的情景从而学会自己安全地驾驶；</p><p>在通过对各种棋谱、棋局的研究学习后，下棋软件能够打败世界一流的棋手；</p><p>等等…</p><h4 id="机器学习的崛起"><a href="#机器学习的崛起" class="headerlink" title="机器学习的崛起"></a>机器学习的崛起</h4><p>机器学习的概念在很早之前就已经出现，例如神经元网络在1943年就已经被提出。 现在，网络技术与自动化技术飞速发展， 出现了大量的数据集可以运用与机器学习算法，例如网络企业的点击记录、医院的电子医疗记录、DNA测序等等；甚至机器学习算法已经被应用于探究人类的学习方式，并试图理解人类的大脑。</p><h4 id="机器学习算法的分类"><a href="#机器学习算法的分类" class="headerlink" title="机器学习算法的分类"></a>机器学习算法的分类</h4><ol><li><p>监督学习(Supervised learning)</p></li><li><p>无监督学习(Unsupervised learning)</p></li><li><p>强化学习(Reinforcement learning)等</p></li></ol><h5 id="一、监督学习-Supervised-learning"><a href="#一、监督学习-Supervised-learning" class="headerlink" title="一、监督学习(Supervised learning)"></a>一、监督学习(Supervised learning)</h5><p>包括回归问题（预测）、根据特征参数分类等。</p><p>举栗子：</p><p>（1）在收集大量关于某地房价与房屋面积的数据后，可以大致判断出房价与面积成线性关系，并能预测出数据中未包含的面积大小对应的大致的房价。</p><p><img src="/images/pasted-0.png" alt="upload successful"></p><p>（2）在收集到大量某肿瘤病人的数据（如年龄与肿瘤大小）后，对病人的病情（如肿瘤良性恶性）进行输入，得到相关模型。之后的病人就可以根据其年龄、肿瘤大小直接预测出其所在分类（良性恶性）。</p><p>总结：在监督学习中，对于数据集中的每个数据， 都有相应的正确答案，（训练集） 算法就是基于这些来做出预测。例如上面的房价和肿瘤性质。</p><p>回归问题即预测一个连续值输出，分类问题即预测一个离散值输出。</p><h5 id="二、无监督学习-Unsupervised-learning"><a href="#二、无监督学习-Unsupervised-learning" class="headerlink" title="二、无监督学习(Unsupervised learning)"></a>二、无监督学习(Unsupervised learning)</h5><p>无监督学习中没有监督学习数据集的属性、标签等，所有的数据都是一样的。</p><p>例如，</p><p>（1）聚类算法，在大量的没有明确标签的数据中，一些数据有着类似的特征或结构，通过聚类算法可以自动的找到这些数据中的类型。</p><p><img src="/images/pasted-1.png" alt="upload successful"></p><p>例如DNA测序，测到的数值量非常大，不同DNA段可能有类似的功能与性质，通过聚类算法找到它们相似的性质并进</p><p>行分类。</p><p><img src="/images/pasted-2.png" alt="upload successful"></p><p>（2）鸡尾酒宴问题：当两个人在同时说话时，有多个话筒分别在不同的位置对他们的声音进行采集，得到多个样本。经过算法处理后可以将两个人同时说话的声音分离开来（奇异值分解）。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>对机器算法有了大概的认知与了解，对与监督学习与无监督学习的异同有了清楚的认识，能够正确地判断出某例是属于哪一类问题</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">lsvih</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">138</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">165</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/lsvih" title="GitHub &rarr; https://github.com/lsvih" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:lsvih@qq.com" title="E-Mail &rarr; mailto:lsvih@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a></span></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2018</span> <span class="with-love" id="animate"><i class="fa fa-"></i> </span><span class="author" itemprop="copyrightHolder">lsvih</span></div><div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.5.0</div><div class="footer-custom"><a target="_blank" rel="external nofollow" href="http://www.miitbeian.gov.cn">京ICP备18029472号</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/reading_progress/reading_progress.js"></script><script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/affix.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.5.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script><link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css"><script src="/lib/algolia-instant-search/instantsearch.min.js"></script><script src="/js/src/algolia-search.js?v=6.5.0"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>